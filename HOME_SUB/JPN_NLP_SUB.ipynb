{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/chizuchizu/IOAI/blob/main/HOME_SUB/JPN_NLP_SUB.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dEDVuanhJGuV"
      },
      "outputs": [],
      "source": [
        "# ====================================================\n",
        "# CFG\n",
        "# ====================================================\n",
        "\n",
        "class CFG:\n",
        "    num_workers=4\n",
        "    project = \"IOAI_Task2_classification\"\n",
        "    name = \"chizu_010_task2_complete\"\n",
        "\n",
        "    # pseudo_base_model_name = \"ioai2024japan/redrock_015_task2_finetune\"\n",
        "    base_model_name = \"google-bert/bert-base-multilingual-uncased\"\n",
        "    base_tokenizer_name = \"google-bert/bert-base-multilingual-uncased\"\n",
        "\n",
        "    # None -> training, otherwise -> load the model\n",
        "    # pretrained_model_name = None # \"ioai2024japan/redrock_005_task2_pretrain_wandb\"\n",
        "    pretrained_model_name = None # \"/content/chizu_004_task2_complete_5e-5_pretrain\"\n",
        "    # tokenizer_name = \"google-bert/bert-base-multilingual-uncased\"\n",
        "    num_classes = 5\n",
        "\n",
        "    # training\n",
        "    pretrain_epochs = 2\n",
        "    classification_epochs = 30\n",
        "    mlm_probability = 0.15\n",
        "\n",
        "    scheduler='linear' # ['ReduceLROnPlateau', 'CosineAnnealingLR', 'CosineAnnealingWarmRestarts']\n",
        "\n",
        "    lr = 5e-5\n",
        "\n",
        "    # dataset\n",
        "    max_length = 256\n",
        "\n",
        "    train_batch_size = 32\n",
        "    eval_batch_size = 32\n",
        "\n",
        "    seed=42\n",
        "    train=True\n",
        "\n",
        "    pseudo_size = 60000\n",
        "    pseudo_select_size = 1500\n",
        "\n",
        "    if_wandb = False\n",
        "\n",
        "# for wandb\n",
        "cfg = dict(vars(CFG))\n",
        "cfg = {k: v for k, v in cfg.items() if \"__\" not in k}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sV85hgL0yxn0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "305de6f3-9aae-4a2d-be34-415fd24a7fc1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torch==2.3.0\n",
            "  Downloading torch-2.3.0-cp310-cp310-manylinux1_x86_64.whl.metadata (26 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0) (3.15.4)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0) (1.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0) (2024.6.1)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch==2.3.0)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch==2.3.0)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch==2.3.0)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch==2.3.0)\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch==2.3.0)\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch==2.3.0)\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch==2.3.0)\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch==2.3.0)\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch==2.3.0)\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-nccl-cu12==2.20.5 (from torch==2.3.0)\n",
            "  Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch==2.3.0)\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting triton==2.3.0 (from torch==2.3.0)\n",
            "  Downloading triton-2.3.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.4 kB)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch==2.3.0)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.6.20-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch==2.3.0) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch==2.3.0) (1.3.0)\n",
            "Downloading torch-2.3.0-cp310-cp310-manylinux1_x86_64.whl (779.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m779.1/779.1 MB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hUsing cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
            "Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Downloading triton-2.3.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (168.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m168.1/168.1 MB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.6.20-py3-none-manylinux2014_x86_64.whl (19.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.7/19.7 MB\u001b[0m \u001b[31m87.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: triton, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, torch\n",
            "  Attempting uninstall: triton\n",
            "    Found existing installation: triton 2.3.1\n",
            "    Uninstalling triton-2.3.1:\n",
            "      Successfully uninstalled triton-2.3.1\n"
          ]
        }
      ],
      "source": [
        "from google.colab import userdata\n",
        "\n",
        "read_access_token = userdata.get('hf_read')\n",
        "write_access_token = userdata.get('hf_write')\n",
        "\n",
        "import importlib\n",
        "import torch, transformers\n",
        "\n",
        "if '2.3.0' not in torch.__version__:\n",
        "  !pip install torch==2.3.0\n",
        "if transformers.__version__!='4.41.2':\n",
        "  !pip install transformers==4.41.2\n",
        "\n",
        "if importlib.util.find_spec('datasets') is None:\n",
        "  !pip install datasets==2.18.0s\n",
        "  !pip install evaluate==0.4.2\n",
        "  !pip install accelerate -U\n",
        "\n",
        "if importlib.util.find_spec('wandb') is None:\n",
        "  !pip install wandb -q\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.functional import F\n",
        "import torch.cuda.amp as amp\n",
        "\n",
        "import torchvision\n",
        "from torchvision import datasets, transforms, models\n",
        "\n",
        "from tqdm.auto import tqdm\n",
        "from transformers import DataCollatorForLanguageModeling\n",
        "\n",
        "from transformers import AutoModelForSequenceClassification, AutoTokenizer, DataCollatorWithPadding, get_scheduler, BertForMaskedLM, BertTokenizer\n",
        "\n",
        "from datasets import load_dataset, Dataset, DatasetDict, concatenate_datasets\n",
        "\n",
        "import evaluate\n",
        "\n",
        "import wandb\n",
        "\n",
        "from joblib import Parallel, delayed\n",
        "\n",
        "\n",
        "from huggingface_hub import login\n",
        "\n",
        "if CFG.if_wandb:\n",
        "    wandb.login(key=userdata.get('wandb_token'))\n",
        "\n",
        "login(token=read_access_token)\n",
        "\n",
        "# brahmi_to_devanagari = {\n",
        "#     '𑀓': 'क', '𑀔': 'ख', '𑀕': 'ग', '𑀖': 'घ', '𑀗': 'ङ', '𑀘': 'च', '𑀙': 'छ',\n",
        "#     '𑀚': 'ज', '𑀛': 'झ', '𑀜': 'ञ', '𑀝': 'ट', '𑀞': 'ठ', '𑀟': 'ड', '𑀠': 'ढ',\n",
        "#     '𑀡': 'ण', '𑀢': 'त', '𑀣': 'थ', '𑀤': 'द', '𑀥': 'ध', '𑀦': 'न', '𑀧': 'प',\n",
        "#     '𑀨': 'फ', '𑀩': 'ब', '𑀪': 'भ', '𑀫': 'म', '𑀬': 'य', '𑀭': 'र', '𑀮': 'ल',\n",
        "#     '𑀯': 'व', '𑀰': 'श', '𑀱': 'ष', '𑀲': 'स', '𑀳': 'ह', '𑁦':'०', '𑁣': '90'\n",
        "# }\n",
        "\n",
        "# def transliterate_brahmi_to_devanagari(text):\n",
        "#     transliterated_text = ''\n",
        "#     for char in text:\n",
        "#         if char in brahmi_to_devanagari:\n",
        "#             transliterated_text += brahmi_to_devanagari[char]\n",
        "#         else:\n",
        "#             transliterated_text += char\n",
        "#     return transliterated_text\n",
        "\n",
        "f1 = evaluate.load(\"f1\")\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    predictions, labels = eval_pred\n",
        "    predictions = np.argmax(predictions, axis=1)\n",
        "    return f1.compute(predictions=predictions, references=labels, average='macro')\n",
        "\n",
        "def to_device(batch, device):\n",
        "    output = {}\n",
        "    for k, v in batch.items():\n",
        "        try:\n",
        "            output[k] = v.to(device)\n",
        "        except:\n",
        "            output[k] = v\n",
        "    return output\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MDkb_0oWkPcn"
      },
      "outputs": [],
      "source": [
        "def train_tokenizer(raw_dataset):\n",
        "    train_corpus = []\n",
        "\n",
        "    num_cores = 8\n",
        "\n",
        "    # train_corpus = Parallel(n_jobs=num_cores)(\n",
        "    #     delayed(text) for text in tqdm(raw_dataset['train'][\"text\"])\n",
        "    # )\n",
        "    train_corpus = [text for text in raw_dataset['train'][\"text\"]]\n",
        "\n",
        "    base_tokenizer = AutoTokenizer.from_pretrained(CFG.base_tokenizer_name)\n",
        "\n",
        "    tokenizer = base_tokenizer.train_new_from_iterator(train_corpus, base_tokenizer.vocab_size)\n",
        "\n",
        "    return tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WLX-zdDfdTXy"
      },
      "outputs": [],
      "source": [
        "def train_one_epoch(model, scheduler, train_loader, optimizer, fp16=False):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    progress_bar = tqdm(train_loader, dynamic_ncols=True)\n",
        "    scaler = torch.cuda.amp.GradScaler()\n",
        "\n",
        "    for step, batch in enumerate(progress_bar):\n",
        "        batch = to_device(batch, \"cuda\")\n",
        "\n",
        "        if fp16:\n",
        "            with amp.autocast():\n",
        "                outputs = model(\n",
        "                    input_ids=batch[\"input_ids\"],\n",
        "                    attention_mask=batch[\"attention_mask\"],\n",
        "                    token_type_ids=batch[\"token_type_ids\"],\n",
        "                    labels=batch[\"labels\"],\n",
        "                )\n",
        "                loss = outputs.loss\n",
        "\n",
        "            # Scale loss for fp16 training\n",
        "            scaler.scale(loss).backward()\n",
        "\n",
        "            # Optimizer step with gradient scaling\n",
        "            scaler.step(optimizer)\n",
        "            scaler.update()\n",
        "            optimizer.zero_grad()\n",
        "            scheduler.step()\n",
        "        else:\n",
        "            outputs = model(\n",
        "                input_ids=batch[\"input_ids\"],\n",
        "                attention_mask=batch[\"attention_mask\"],\n",
        "                token_type_ids=batch[\"token_type_ids\"],\n",
        "                labels=batch[\"labels\"],\n",
        "            )\n",
        "            loss = outputs.loss\n",
        "            loss.backward()\n",
        "\n",
        "            optimizer.step()\n",
        "            scheduler.step()\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "        if CFG.if_wandb:\n",
        "            wandb.log(\n",
        "                {\n",
        "                    \"train_loss\": loss,\n",
        "                    \"lr\": optimizer.param_groups[0][\"lr\"],\n",
        "                    \"step\": step,\n",
        "                }\n",
        "            )\n",
        "\n",
        "        text = f\"step {step}, loss: {loss:.5f}\"\n",
        "        progress_bar.set_description(text)\n",
        "\n",
        "def evaluate_model(model, eval_loader):\n",
        "    model.eval()\n",
        "    predictions = []\n",
        "    labels = []\n",
        "    for batch in eval_loader:\n",
        "        batch = to_device(batch, \"cuda\")\n",
        "        with torch.no_grad():\n",
        "            outputs = model(\n",
        "                input_ids=batch[\"input_ids\"],\n",
        "                attention_mask=batch[\"attention_mask\"],\n",
        "                token_type_ids=batch[\"token_type_ids\"],\n",
        "                labels=batch[\"labels\"],\n",
        "            )\n",
        "\n",
        "        logits = outputs.logits\n",
        "        prediction = torch.argmax(logits, dim=-1)\n",
        "        predictions.append(prediction.cpu().numpy())\n",
        "        labels.append(batch[\"labels\"].cpu().numpy())\n",
        "\n",
        "    predictions = np.concatenate(predictions)\n",
        "    labels = np.concatenate(labels)\n",
        "    f1_score = f1.compute(predictions=predictions, references=labels, average='macro')\n",
        "    return f1_score, predictions\n",
        "\n",
        "def test_model(model, eval_loader):\n",
        "    model.eval()\n",
        "    predictions = []\n",
        "    labels = []\n",
        "    for batch in eval_loader:\n",
        "        batch = to_device(batch, \"cuda\")\n",
        "        with torch.no_grad():\n",
        "            outputs = model(\n",
        "                input_ids=batch[\"input_ids\"],\n",
        "                attention_mask=batch[\"attention_mask\"],\n",
        "                token_type_ids=batch[\"token_type_ids\"],\n",
        "            )\n",
        "\n",
        "        logits = outputs.logits\n",
        "        prediction = torch.argmax(logits, dim=-1)\n",
        "        predictions.append(prediction.cpu().numpy())\n",
        "\n",
        "    predictions = np.concatenate(predictions)\n",
        "    return predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1SSBvno3jCGv"
      },
      "outputs": [],
      "source": [
        "def pretrain(raw_dataset, tokenizer, transform_raw, fp16=True):\n",
        "    print(\"=== Pretrain ===\")\n",
        "\n",
        "    data_collator = DataCollatorForLanguageModeling(\n",
        "        tokenizer=tokenizer,\n",
        "        mlm=True,\n",
        "        mlm_probability=CFG.mlm_probability\n",
        "    )\n",
        "\n",
        "    tokenized_data = raw_dataset.with_transform(transform_raw)\n",
        "\n",
        "    train_dataset = tokenized_data[\"train\"]\n",
        "\n",
        "    train_loader = DataLoader(\n",
        "        train_dataset,\n",
        "        batch_size=CFG.train_batch_size,\n",
        "        num_workers=0,\n",
        "        pin_memory=True,\n",
        "        shuffle=True,\n",
        "        drop_last=True,\n",
        "        collate_fn=data_collator,\n",
        "    )\n",
        "\n",
        "    model = BertForMaskedLM.from_pretrained(\n",
        "        CFG.base_model_name\n",
        "    ).cuda()\n",
        "\n",
        "    num_training_steps = CFG.pretrain_epochs * len(train_loader)\n",
        "    optimizer = optim.AdamW(model.parameters(), lr=CFG.lr)\n",
        "    scheduler = get_scheduler(name=CFG.scheduler, optimizer=optimizer, num_warmup_steps=0, num_training_steps=num_training_steps)\n",
        "\n",
        "    for i in range(CFG.pretrain_epochs):\n",
        "        train_one_epoch(model, scheduler, train_loader, optimizer, fp16)\n",
        "        print(f'Epoch {i+1}')\n",
        "\n",
        "    model_path = f\"{CFG.name}_pretrain\"\n",
        "    model.save_pretrained(model_path)\n",
        "    return model_path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jXAjWlZFmBlm"
      },
      "outputs": [],
      "source": [
        "def finetine(base_model, train_dataset, eval_dataset, device, fp16=False):\n",
        "    print(\"=== Finetune ===\")\n",
        "    model = AutoModelForSequenceClassification.from_pretrained(\n",
        "        base_model, num_labels=CFG.num_classes\n",
        "    ).cuda()\n",
        "\n",
        "    train_loader = DataLoader(\n",
        "        train_dataset,\n",
        "        batch_size=CFG.train_batch_size,\n",
        "        num_workers=0,\n",
        "        pin_memory=True,\n",
        "        shuffle=True,\n",
        "        drop_last=True,\n",
        "    )\n",
        "\n",
        "    eval_loader = DataLoader(\n",
        "        eval_dataset,\n",
        "        batch_size=CFG.eval_batch_size,\n",
        "        num_workers=0,\n",
        "        pin_memory=True,\n",
        "        shuffle=False,\n",
        "        drop_last=False,\n",
        "    )\n",
        "\n",
        "    num_training_steps = CFG.classification_epochs * len(train_loader)\n",
        "    optimizer = optim.AdamW(model.parameters(), lr=CFG.lr)\n",
        "    scheduler = get_scheduler(name=CFG.scheduler, optimizer=optimizer, num_warmup_steps=0, num_training_steps=num_training_steps)\n",
        "\n",
        "    best_f1 = 0.0\n",
        "    best_model_path = None\n",
        "\n",
        "    model.to(device)\n",
        "    for i in range(CFG.classification_epochs):\n",
        "        train_one_epoch(model, scheduler, train_loader, optimizer)\n",
        "        f1_score, _ = evaluate_model(model, eval_loader)\n",
        "        f1_score = f1_score[\"f1\"]\n",
        "\n",
        "        if f1_score > best_f1:\n",
        "            best_f1 = f1_score\n",
        "            best_model_path = f\"{CFG.name}_finetune_epoch_{i+1}\"\n",
        "            model.save_pretrained(best_model_path)\n",
        "\n",
        "\n",
        "        if CFG.if_wandb:\n",
        "            wandb.log(\n",
        "                {\n",
        "                    \"epoch\": i+1,\n",
        "                    \"f1\": f1_score\n",
        "                }\n",
        "            )\n",
        "        print(f'Epoch {i+1} {f1_score}')\n",
        "\n",
        "    # model_path = f\"{CFG.name}_finetune\"\n",
        "    # model.save_pretrained(model_path)\n",
        "    return best_model_path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3P_oA7-FPATK"
      },
      "outputs": [],
      "source": [
        "def up_to_hub(model_name, model_path, tokenizer):\n",
        "    model = AutoModelForSequenceClassification.from_pretrained(\n",
        "        model_path, num_labels=CFG.num_classes\n",
        "    )\n",
        "    model.push_to_hub(\n",
        "        f\"ioai2024japan/{model_name}\",\n",
        "        token=write_access_token, private=True\n",
        "    )\n",
        "    tokenizer.push_to_hub(\n",
        "        f\"ioai2024japan/{model_name}\",\n",
        "        token=write_access_token, private=True\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cHRDuKNBj5IW"
      },
      "outputs": [],
      "source": [
        "def main():\n",
        "    raw_dataset = load_dataset('InternationalOlympiadAI/NLP_problem_raw', token=read_access_token)\n",
        "    classification_dataset = load_dataset('InternationalOlympiadAI/NLP_problem', token=read_access_token)\n",
        "\n",
        "    device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "\n",
        "    # Train tokenizer to Lang X\n",
        "    tokenizer = train_tokenizer(raw_dataset)\n",
        "\n",
        "    # for raw set\n",
        "    def transform_raw(example_batch):\n",
        "        # example_batch[\"text\"] = [transliterate_brahmi_to_devanagari(x) for x in example_batch[\"text\"]]\n",
        "        inputs = tokenizer([x for x in example_batch[\"text\"]],  truncation=True, max_length=CFG.max_length, padding=\"max_length\", return_tensors=\"pt\")\n",
        "        return inputs\n",
        "\n",
        "    # for problem set\n",
        "    def transform(example_batch):\n",
        "        # example_batch[\"text\"] = [transliterate_brahmi_to_devanagari(x) for x in example_batch[\"text\"]]\n",
        "        inputs = tokenizer([x for x in example_batch[\"text\"]],  truncation=True, max_length=CFG.max_length, padding=\"max_length\", return_tensors=\"pt\")\n",
        "        inputs[\"labels\"] = example_batch[\"label\"]\n",
        "        return inputs\n",
        "\n",
        "    tokenized_data = classification_dataset.with_transform(transform)\n",
        "\n",
        "    train_dataset = tokenized_data[\"train\"]\n",
        "    eval_dataset = tokenized_data[\"dev\"]\n",
        "\n",
        "    # Continual Pre-Training of MLM\n",
        "    if CFG.pretrained_model_name is None:\n",
        "        if CFG.if_wandb:\n",
        "            wandb.init(\n",
        "                name=CFG.name,\n",
        "                project=\"IOAI_Task2_pretrain\",\n",
        "                config=cfg\n",
        "            )\n",
        "        pretrained_model_path = pretrain(raw_dataset, tokenizer, transform_raw, fp16=True)\n",
        "        up_to_hub(f\"{CFG.name}_pretrain\", pretrained_model_path, tokenizer)\n",
        "    else:\n",
        "        pretrained_model_path = CFG.pretrained_model_name\n",
        "\n",
        "    if CFG.if_wandb:\n",
        "        wandb.init(\n",
        "            name=CFG.name,\n",
        "            project=\"IOAI_Task2_finetune\",\n",
        "            config=cfg\n",
        "        )\n",
        "\n",
        "    # Finetune with normal dataset\n",
        "    finetuned_model_path = finetine(pretrained_model_path, train_dataset, eval_dataset, device)\n",
        "\n",
        "    if CFG.if_wandb:\n",
        "        wandb.finish()\n",
        "\n",
        "    # # Get pseudo label\n",
        "    # pseudo_data, confidences = pseudo_get_data(raw_dataset, transform_raw, finetuned_model_path, device)\n",
        "    # pseudo_labeled_tokens = pseudo_data.with_transform(transform)\n",
        "\n",
        "    # combined_train_dataset = concatenate_datasets([pseudo_labeled_tokens, train_dataset])\n",
        "\n",
        "    # # Finetune with pseudo dataset and normal dataset\n",
        "    # final_model_name = finetine(pretrained_model_path, combined_train_dataset, eval_dataset, device)\n",
        "\n",
        "    return finetuned_model_path, tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zs02eqO9jQBx"
      },
      "outputs": [],
      "source": [
        "final_model_name, tokenizer = main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PJhxgL4TuU9F"
      },
      "outputs": [],
      "source": [
        "up_to_hub(CFG.name, final_model_name, tokenizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lMNqHsF0olYA"
      },
      "outputs": [],
      "source": [
        "model = AutoModelForSequenceClassification.from_pretrained(\n",
        "    final_model_name, num_labels=CFG.num_classes\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gg3XyWrXR4u0"
      },
      "outputs": [],
      "source": [
        "# run the trained model on a dev/test split\n",
        "classification_dataset = load_dataset('InternationalOlympiadAI/NLP_problem', token=read_access_token)\n",
        "\n",
        "def transform_raw(example_batch):\n",
        "    # example_batch[\"text\"] = [transliterate_brahmi_to_devanagari(x) for x in example_batch[\"text\"]]\n",
        "    inputs = tokenizer([x for x in example_batch[\"text\"]],  truncation=True, max_length=CFG.max_length, padding=\"max_length\", return_tensors=\"pt\")\n",
        "    return inputs\n",
        "\n",
        "data_split = \"dev\"\n",
        "tokenized_data = classification_dataset.with_transform(transform_raw)\n",
        "test_dataset = tokenized_data[data_split]\n",
        "\n",
        "test_loader = DataLoader(\n",
        "    test_dataset,\n",
        "    batch_size=CFG.eval_batch_size,\n",
        "    num_workers=0,\n",
        "    pin_memory=True,\n",
        "    shuffle=False,\n",
        "    drop_last=False,\n",
        ")\n",
        "model.cuda()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ro1IVGKtfyvp"
      },
      "outputs": [],
      "source": [
        "predictions = test_model(model, test_loader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s9npUvQ3R7cr"
      },
      "outputs": [],
      "source": [
        "# write the predictions to a file\n",
        "with open('{}_predictions.txt'.format(data_split), 'w') as outfile:\n",
        "  outfile.write('\\n'.join([str(p) for p in predictions.tolist()]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RFe6sNrcgJa_"
      },
      "outputs": [],
      "source": [
        "predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RAZPPT9KK9Sg"
      },
      "outputs": [],
      "source": [
        "# UPDATE THIS CELL ACCORDINGLY\n",
        "\n",
        "# define a funciton to load your tokenizer and model from a HF path\n",
        "# the path variables can be strings or lists of strings (for ensemble solutions)\n",
        "def load_model(path_to_tokenizer, path_to_model, token):\n",
        "  # Example:\n",
        "  tokenizer = AutoTokenizer.from_pretrained(path_to_tokenizer, token=token)\n",
        "  model = AutoModelForSequenceClassification.from_pretrained(path_to_model, token=token)\n",
        "  model.eval()\n",
        "\n",
        "  return tokenizer, model\n",
        "\n",
        "# define a \"predict\" function that takes the model and a list of input strings\n",
        "# and returns the outputs as a list of integer classes\n",
        "def predict(tokenizer, model, input_texts):\n",
        "  #Example:\n",
        "  predictions = []\n",
        "  for input_text in input_texts:\n",
        "\n",
        "    # input_ids = tokenizer(input_text, return_tensors=\"pt\")\n",
        "    # devanagari_text = transliterate_brahmi_to_devanagari(input_text)\n",
        "    input_ids = tokenizer(input_text, return_tensors=\"pt\", truncation=True, max_length=CFG.max_length, padding=\"max_length\")\n",
        "        # example_batch[\"text\"] = [transliterate_brahmi_to_devanagari(x) for x in example_batch[\"text\"]]\n",
        "        # inputs = tokenizer([x for x in example_batch[\"text\"]],  truncation=True, max_length=CFG.max_length, padding=\"max_length\", return_tensors=\"pt\")\n",
        "    with torch.no_grad():\n",
        "      logits = model(**input_ids).logits\n",
        "\n",
        "    predictions.append(logits.argmax().item())\n",
        "\n",
        "  return predictions\n",
        "\n",
        "\n",
        "# set variables\n",
        "path_to_model = \"ioai2024japan/chizu_010_task2_complete\" # can be a list instead\n",
        "path_to_tokenizer = \"ioai2024japan/chizu_010_task2_complete\" # can be a list instead\n",
        "model_access_token = read_access_token # a fine-grained token with read rights for your model repository"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-myykogdK-DE"
      },
      "outputs": [],
      "source": [
        "# DO NOT CHANGE THIS CELL!!!\n",
        "\n",
        "tokenizer, model = load_model(path_to_tokenizer, path_to_model, token=model_access_token)\n",
        "\n",
        "test_data = load_dataset(\"InternationalOlympiadAI/NLP_problem_test\")['test']['text']\n",
        "\n",
        "predictions = predict(tokenizer, model, test_data)\n",
        "\n",
        "with open('test_predictions.txt', 'w') as outfile:\n",
        "  outfile.write('\\n'.join([str(p) for p in predictions]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-sToEYVZ3dmT"
      },
      "outputs": [],
      "source": [
        "def terminate_session():\n",
        "    # Terminate this session\n",
        "\n",
        "    from google.colab import runtime\n",
        "    runtime.unassign()\n",
        "\n",
        "terminate_session()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}