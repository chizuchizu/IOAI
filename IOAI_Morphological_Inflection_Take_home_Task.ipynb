{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w4dPSnSw59x9"
      },
      "source": [
        "# Morphological Inflection in Navajo\n",
        "\n",
        "## Problem Description\n",
        "\n",
        "Morphological inflection is a task in computational linguistics wherein the correct form of a word has to be generated from a lemma (base form) and a target morphosyntactic specification, e.g.\n",
        "\n",
        "\n",
        "```\n",
        "alzhish  +  V;IND;PFV;NOM(2,SG) -> íínílzhiizh\n",
        "```\n",
        "\n",
        "\n",
        "where *alzhish* is the Navajo word for 'dance', and *íínílzhiizh* is the second [2] person , singular [SG], nominative [NOM] form of the verb [V] in the indicative [IND] mood and perfect [PFV] aspect. You can find out more about the annotation schema used for the morphosyntactic specification of the target forms [here](https://unimorph.github.io/doc/unimorph-schema.pdf).\n",
        "\n",
        "This task finds application in the construction of resources for language acquisition, in statistical machine translation, and in the computational study of language.\n",
        "\n",
        "The difficulty of the task for a given language depends on the size and complexity of the inflectional paradigm of the language. A large paradigm, which distinguishes between many morphosyntactic properties (number, gender, case, aspect, etc.) and many values for these properties, requires extensive machine learning from sizeable data. While inflectional paragims tend to be highly regular, they can vary in complexity, with many factors guiding which exact morpheme should attach to a given root to mark a specific morphosyntactic property. And irregular forms can often be found too, which cannot be predicted and have to be memorized instead.\n",
        "\n",
        "Here, we ask you to train a machine learning model to perform morphological inflection in Navajo.\n",
        "\n",
        "## Your Task\n",
        "\n",
        "The code below is a near reimplementation of the approach to moprhological inflection presented in [Wu et al.](https://aclanthology.org/2021.eacl-main.163.pdf). The reimplementation uses high-level API from the *transformers* Python library. High-level APIs are convenient as they save us a lot of code-writing, but they also obscure certain aspects of the implementation.\n",
        "\n",
        "The approach of Wu and colleagues achieves a score of 52.1% for Navajo (as reported [here]()), while the score of the model below lingers about 3 percentage points behind. Study the approach of Wu and colleagues as described in their article to find out what is missing from the model below and make the necessary changes to reproduce their result.   \n",
        "\n",
        "Notice that due to the stochasticity of the model initialization and training, slight deviations from the expected score are admissible (within 0.5 percentage points).\n",
        "\n",
        "### Deliverables:\n",
        "* code\n",
        "* model accuracy on the test set\n",
        "* a report of up to 300 words describing the changes made to the model architecture, data processing, training procedure etc.\n",
        "    * If you do not succeed in reproducing the expected score, comment on what the reason may be.\n",
        "\n",
        "\n",
        "## Technical Specifications\n",
        "\n",
        "* All team solutions should be submitted as a modified and compiled copy of this base notebook.\n",
        "* Do not change cells starting with the `###DO NOT CHANGE THIS CELL###` comment.\n",
        "* You can use any platform to carry out the development of your model, but for the final submission you have to integrate your solution back into this notebook, adding code to install all dependencies and making sure that your model takes no longer than 8 hours to train.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yui_SWs9mXx-",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "! pip install -U accelerate\n",
        "! pip install -U transformers\n",
        "! pip install -U evaluate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R_dknIv5LIsz",
        "outputId": "9af7f8de-345a-4d7c-ac08-a33ffb967026"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloaded 10000 training data samples and 1000 dev samples.\n",
            "Raw data sample: adiłhash\tV;IND;PFV;NOM(3,GRPL)\tdaʼdiłhash\n"
          ]
        }
      ],
      "source": [
        "###DO NOT CHANGE THIS CELL###\n",
        "\"\"\"\n",
        "Download the train and dev data\n",
        "\"\"\"\n",
        "\n",
        "import requests\n",
        "\n",
        "def download_data(path):\n",
        "        response = requests.get(path)\n",
        "        return response.text.strip().split('\\n')\n",
        "\n",
        "train_data_path = 'https://raw.githubusercontent.com/sigmorphon/2023InflectionST/main/part1/data/nav.trn'\n",
        "dev_data_path = 'https://raw.githubusercontent.com/sigmorphon/2023InflectionST/main/part1/data/nav.dev'\n",
        "\n",
        "data = {}\n",
        "data['train'] = download_data(train_data_path)\n",
        "data['dev'] = download_data(dev_data_path)\n",
        "\n",
        "print('Downloaded {} training data samples and {} dev samples.'.format(len(data['train']), len(data['dev'])))\n",
        "print('Raw data sample:', data['train'][54])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D0HpSXXVLIs0"
      },
      "source": [
        "# Task 1\n",
        "Modify the code below to achieve a full reimplementation of the approach of Wu and colleagues."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b1tJreU_6Y2Q",
        "outputId": "db497d03-bbfd-4188-c889-c9b8330b884e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Preprocessed data sample: ('adiłhash <V><IND><PFV><NOM><3><GRPL>', 'daʼdiłhash')\n"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "DATA PREPROCESSING\n",
        "\"\"\"\n",
        "\n",
        "import re\n",
        "import regex\n",
        "\n",
        "def parse_tag(tag):\n",
        "    tag = re.sub(r\"\\)|\\(|,|;\", ' ', tag).split()\n",
        "    return ''.join(['<{}>'.format(t) for t in tag])\n",
        "\n",
        "def preprocess_data(raw_data):\n",
        "        preprocessed_data = []\n",
        "        for line in raw_data:\n",
        "          lemma, tag, target = line.split('\\t')\n",
        "          preprocessed_data.append(('{} {}'.format(lemma, parse_tag(tag)),target))\n",
        "        return preprocessed_data\n",
        "\n",
        "data['train'] = preprocess_data(data['train'])\n",
        "data['dev'] = preprocess_data(data['dev'])\n",
        "\n",
        "print('Preprocessed data sample:', data['train'][54])\n",
        "\n",
        "chars = set(list(''.join([''.join([d[0].split()[0], d[1]]) for d in data['train']])))\n",
        "char2id = { char: i for i, char in enumerate(chars)}\n",
        "tags = list(set(sum([regex.findall(r\"<[A-Za-z0-9]*>\",d[0]) for d in data['train']], [])))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XO-qTiG5LIs0",
        "outputId": "5d526c54-67f9-489f-ef59-c53fbdfaa313"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tokenization example: ['a', 'd', 'i', 'ł', 'h', 'a', 's', 'h', ' ', '<V>', '<IND>', '<PFV>', '<NOM>', '<3>', '<GRPL>']\n"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "TOKENIZER\n",
        "Input and output word forms are processed one character at a time, while morphosyntactic features are treated as special atomic tokens.\n",
        "\"\"\"\n",
        "\n",
        "import json\n",
        "from pathlib import Path\n",
        "from typing import Optional, Tuple, Dict, List\n",
        "\n",
        "from transformers import PreTrainedTokenizer\n",
        "from transformers.tokenization_utils import AddedToken\n",
        "\n",
        "import warnings\n",
        "\n",
        "class CustomTokenizer(PreTrainedTokenizer):\n",
        "\n",
        "    model_input_names = [\"input_ids\", \"attention_mask\"]\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        vocab: Dict[str, int],\n",
        "        bos_token=\"<s>\",\n",
        "        eos_token=\"</s>\",\n",
        "        unk_token=\"<unk>\",\n",
        "        pad_token=\"<pad>\",\n",
        "        **kwargs,\n",
        "    ) -> None:\n",
        "        # Add extra_ids to the special token list\n",
        "\n",
        "        self.__token_ids = vocab\n",
        "        self.__id_tokens: Dict[int, str] = {value: key for key, value in vocab.items()}\n",
        "\n",
        "        pad_token = AddedToken(pad_token, lstrip=False, rstrip=False) if isinstance(pad_token, str) else pad_token\n",
        "        bos_token = AddedToken(bos_token, lstrip=False, rstrip=False) if isinstance(bos_token, str) else bos_token\n",
        "        eos_token = AddedToken(eos_token, lstrip=False, rstrip=False) if isinstance(eos_token, str) else eos_token\n",
        "        unk_token = AddedToken(unk_token, lstrip=False, rstrip=False) if isinstance(unk_token, str) else unk_token\n",
        "        self._added_tokens_decoder = {0: pad_token, 1: bos_token, 2: eos_token, 3: unk_token}\n",
        "        self.offset = len(self._added_tokens_decoder)\n",
        "\n",
        "        super().__init__(\n",
        "            bos_token=bos_token,\n",
        "            eos_token=eos_token,\n",
        "            unk_token=unk_token,\n",
        "            pad_token=pad_token,\n",
        "            **kwargs,\n",
        "        )\n",
        "\n",
        "    @property\n",
        "    def vocab_size(self) -> int:\n",
        "        return len(self.__token_ids)\n",
        "\n",
        "    def get_vocab(self) -> Dict[str, int]:\n",
        "        vocab = {self.convert_ids_to_tokens(i): i for i in range(self.vocab_size + self.offset)}\n",
        "        vocab.update(self.added_tokens_encoder)\n",
        "        return vocab\n",
        "\n",
        "    def _add_eos(self, token_ids: List[int]) -> List[int]:\n",
        "        return token_ids + [self.eos_token_id]\n",
        "\n",
        "    def create_token_type_ids_from_sequences(\n",
        "        self, token_ids_0: List[int], token_ids_1: Optional[List[int]] = None\n",
        "    ) -> List[int]:\n",
        "        eos = [self.eos_token_id]\n",
        "\n",
        "        if token_ids_1 is None:\n",
        "            return len(token_ids_0 + eos) * [0]\n",
        "        return len(token_ids_0 + eos + token_ids_1 + eos) * [0]\n",
        "\n",
        "    def build_inputs_with_special_tokens(\n",
        "        self, token_ids_0: List[int], token_ids_1: Optional[List[int]] = None\n",
        "    ) -> List[int]:\n",
        "        token_ids_0 = self._add_eos(token_ids_0)\n",
        "        if token_ids_1 is None:\n",
        "            return token_ids_0\n",
        "        else:\n",
        "            token_ids_1 = self._add_eos(token_ids_1)\n",
        "            return token_ids_0 + token_ids_1\n",
        "\n",
        "    def _tokenize(self, text: str, **kwargs):\n",
        "        return list(text)\n",
        "\n",
        "    def _convert_token_to_id(self, token: str) -> int:\n",
        "        return self.__token_ids[token]+self.offset if token in self.__token_ids else self.unk_token_id\n",
        "\n",
        "    def _convert_id_to_token(self, index: int) -> str:\n",
        "        return self.__id_tokens[index-self.offset] if index-self.offset in self.__id_tokens else self.unk_token\n",
        "\n",
        "    def convert_tokens_to_string(self, tokens):\n",
        "        return \"\".join(tokens)\n",
        "\n",
        "    def save_vocabulary(self, save_directory: str,\n",
        "                        filename_prefix: Optional[str] = None) -> Tuple[str]:\n",
        "        if filename_prefix is None:\n",
        "            filename_prefix = ''\n",
        "        vocab_path = Path(save_directory, filename_prefix + 'vocab.json')\n",
        "        json.dump(self.__token_ids, open(vocab_path, 'w'))\n",
        "        return str(vocab_path),\n",
        "\n",
        "tokenizer = CustomTokenizer(char2id, additional_special_tokens=tags, max_len=100)\n",
        "print('Tokenization example:', tokenizer.tokenize(data['train'][54][0]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5kmN6C1iAxlL"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "MODEL\n",
        "The model is based on the t5 model architecture: a transformer-based model with an encoder\n",
        "and a decoder, trained to take an input sequence (the lemma and the target tag)\n",
        "and to generate an output sequence (the target form).\n",
        "\"\"\"\n",
        "\n",
        "from transformers import T5Config, T5ForConditionalGeneration\n",
        "\n",
        "config = T5Config(d_ff=1024,\n",
        "              d_model=256,\n",
        "              num_layers=4,\n",
        "              num_decoder_layers=4,\n",
        "              num_heads=4,\n",
        "              dropout_rate=0.2,\n",
        "              vocab_size=len(tokenizer))\n",
        "model = T5ForConditionalGeneration(config)\n",
        "model.config.decoder_start_token_id = tokenizer.bos_token_id\n",
        "model.generation_config.decoder_start_token_id = tokenizer.bos_token_id\n",
        "model.generation_config.max_new_tokens = 32\n",
        "model.generation_config.eos_token_id = tokenizer.eos_token_id\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CWQTI68gLIs1"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "UTILITIES\n",
        "\"\"\"\n",
        "\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, data, tokenizer):\n",
        "        self.data = data\n",
        "        self.tokenizer = tokenizer\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "    def __getitem__(self, idx):\n",
        "        input, target = self.data[idx]\n",
        "        return {\"input_ids\": self.tokenizer(input, padding='longest',\n",
        "                                            truncation=True,\n",
        "                                            add_special_tokens=True)['input_ids'],\n",
        "                \"labels\": self.tokenizer(target, padding='longest',\n",
        "                                         truncation=True,\n",
        "                                         add_special_tokens=True)['input_ids']}\n",
        "\n",
        "def postprocess_data(token_ids):\n",
        "    token_ids = np.where(token_ids != -100, token_ids, tokenizer.pad_token_id)\n",
        "    return tokenizer.batch_decode(token_ids, skip_special_tokens=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PixE9lVPLIs1"
      },
      "outputs": [],
      "source": [
        "###DO NOT CHANGE THIS CELL###\n",
        "\"\"\"\n",
        "MAIN METRIC\n",
        "\"\"\"\n",
        "\n",
        "import evaluate\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "def compute_metrics(eval_preds):\n",
        "    metric = evaluate.load(\"exact_match\")\n",
        "    preds, labels = eval_preds\n",
        "    decoded_preds = postprocess_data(preds)\n",
        "    decoded_labels = postprocess_data(labels)\n",
        "\n",
        "    # During development, you can uncomment the lines to see what predictions your model makes\n",
        "    # ks = random.choices(list(range(len(decoded_preds))), k=15)\n",
        "    # print('Predicted:', [decoded_preds[k] for k in ks])\n",
        "    # print('Targets:', [decoded_labels[k] for k in ks])\n",
        "    # print('___________________________________________________________________')\n",
        "\n",
        "    result = metric.compute(predictions=decoded_preds, references=decoded_labels)\n",
        "    return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R0n-bgxSH8HP",
        "outputId": "d8c627b7-9d51-4c03-f519-934682083a00"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='20000' max='20000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [20000/20000 1:02:39, Epoch 800/800]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Exact Match</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>400</td>\n",
              "      <td>2.768400</td>\n",
              "      <td>1.623577</td>\n",
              "      <td>0.032000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>800</td>\n",
              "      <td>1.512300</td>\n",
              "      <td>1.423992</td>\n",
              "      <td>0.137000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1200</td>\n",
              "      <td>1.178200</td>\n",
              "      <td>1.388152</td>\n",
              "      <td>0.160000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1600</td>\n",
              "      <td>1.015200</td>\n",
              "      <td>1.436030</td>\n",
              "      <td>0.209000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2000</td>\n",
              "      <td>0.925100</td>\n",
              "      <td>1.421848</td>\n",
              "      <td>0.221000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2400</td>\n",
              "      <td>0.868800</td>\n",
              "      <td>1.440768</td>\n",
              "      <td>0.238000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2800</td>\n",
              "      <td>0.835900</td>\n",
              "      <td>1.462445</td>\n",
              "      <td>0.279000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3200</td>\n",
              "      <td>0.815400</td>\n",
              "      <td>1.440195</td>\n",
              "      <td>0.296000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3600</td>\n",
              "      <td>0.802400</td>\n",
              "      <td>1.423015</td>\n",
              "      <td>0.321000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4000</td>\n",
              "      <td>0.793900</td>\n",
              "      <td>1.407912</td>\n",
              "      <td>0.363000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4400</td>\n",
              "      <td>0.783100</td>\n",
              "      <td>1.420568</td>\n",
              "      <td>0.328000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4800</td>\n",
              "      <td>0.771500</td>\n",
              "      <td>1.386598</td>\n",
              "      <td>0.379000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5200</td>\n",
              "      <td>0.760300</td>\n",
              "      <td>1.397171</td>\n",
              "      <td>0.377000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5600</td>\n",
              "      <td>0.753500</td>\n",
              "      <td>1.382452</td>\n",
              "      <td>0.372000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6000</td>\n",
              "      <td>0.747500</td>\n",
              "      <td>1.391065</td>\n",
              "      <td>0.405000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6400</td>\n",
              "      <td>0.743500</td>\n",
              "      <td>1.366969</td>\n",
              "      <td>0.399000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6800</td>\n",
              "      <td>0.740100</td>\n",
              "      <td>1.378463</td>\n",
              "      <td>0.423000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7200</td>\n",
              "      <td>0.737300</td>\n",
              "      <td>1.396914</td>\n",
              "      <td>0.408000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7600</td>\n",
              "      <td>0.734900</td>\n",
              "      <td>1.390532</td>\n",
              "      <td>0.384000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8000</td>\n",
              "      <td>0.732800</td>\n",
              "      <td>1.384905</td>\n",
              "      <td>0.407000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8400</td>\n",
              "      <td>0.730800</td>\n",
              "      <td>1.369778</td>\n",
              "      <td>0.409000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8800</td>\n",
              "      <td>0.729400</td>\n",
              "      <td>1.386565</td>\n",
              "      <td>0.419000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9200</td>\n",
              "      <td>0.728200</td>\n",
              "      <td>1.335642</td>\n",
              "      <td>0.418000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9600</td>\n",
              "      <td>0.726900</td>\n",
              "      <td>1.373792</td>\n",
              "      <td>0.433000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10000</td>\n",
              "      <td>0.726100</td>\n",
              "      <td>1.386220</td>\n",
              "      <td>0.406000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10400</td>\n",
              "      <td>0.725000</td>\n",
              "      <td>1.349640</td>\n",
              "      <td>0.437000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10800</td>\n",
              "      <td>0.724500</td>\n",
              "      <td>1.372176</td>\n",
              "      <td>0.460000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>11200</td>\n",
              "      <td>0.723900</td>\n",
              "      <td>1.357036</td>\n",
              "      <td>0.444000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>11600</td>\n",
              "      <td>0.723200</td>\n",
              "      <td>1.383446</td>\n",
              "      <td>0.407000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12000</td>\n",
              "      <td>0.722400</td>\n",
              "      <td>1.415457</td>\n",
              "      <td>0.405000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12400</td>\n",
              "      <td>0.722000</td>\n",
              "      <td>1.349416</td>\n",
              "      <td>0.448000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12800</td>\n",
              "      <td>0.721600</td>\n",
              "      <td>1.349123</td>\n",
              "      <td>0.456000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>13200</td>\n",
              "      <td>0.721000</td>\n",
              "      <td>1.395071</td>\n",
              "      <td>0.441000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>13600</td>\n",
              "      <td>0.720900</td>\n",
              "      <td>1.374795</td>\n",
              "      <td>0.450000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>14000</td>\n",
              "      <td>0.720400</td>\n",
              "      <td>1.379094</td>\n",
              "      <td>0.446000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>14400</td>\n",
              "      <td>0.719900</td>\n",
              "      <td>1.386504</td>\n",
              "      <td>0.442000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>14800</td>\n",
              "      <td>0.719600</td>\n",
              "      <td>1.356190</td>\n",
              "      <td>0.467000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>15200</td>\n",
              "      <td>0.719700</td>\n",
              "      <td>1.395790</td>\n",
              "      <td>0.432000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>15600</td>\n",
              "      <td>0.719100</td>\n",
              "      <td>1.406570</td>\n",
              "      <td>0.434000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>16000</td>\n",
              "      <td>0.718800</td>\n",
              "      <td>1.387578</td>\n",
              "      <td>0.436000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>16400</td>\n",
              "      <td>0.718500</td>\n",
              "      <td>1.406078</td>\n",
              "      <td>0.420000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>16800</td>\n",
              "      <td>0.718400</td>\n",
              "      <td>1.401115</td>\n",
              "      <td>0.433000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>17200</td>\n",
              "      <td>0.718100</td>\n",
              "      <td>1.353011</td>\n",
              "      <td>0.457000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>17600</td>\n",
              "      <td>0.718000</td>\n",
              "      <td>1.346120</td>\n",
              "      <td>0.452000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>18000</td>\n",
              "      <td>0.717800</td>\n",
              "      <td>1.343290</td>\n",
              "      <td>0.465000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>18400</td>\n",
              "      <td>0.717600</td>\n",
              "      <td>1.333486</td>\n",
              "      <td>0.467000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>18800</td>\n",
              "      <td>0.717400</td>\n",
              "      <td>1.358209</td>\n",
              "      <td>0.454000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>19200</td>\n",
              "      <td>0.717100</td>\n",
              "      <td>1.321765</td>\n",
              "      <td>0.466000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>19600</td>\n",
              "      <td>0.717100</td>\n",
              "      <td>1.339502</td>\n",
              "      <td>0.456000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20000</td>\n",
              "      <td>0.716800</td>\n",
              "      <td>1.338070</td>\n",
              "      <td>0.435000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "There were missing keys in the checkpoint model loaded: ['encoder.embed_tokens.weight', 'decoder.embed_tokens.weight', 'lm_head.weight'].\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "TrainOutput(global_step=20000, training_loss=0.8132031356811523, metrics={'train_runtime': 3760.6997, 'train_samples_per_second': 2127.264, 'train_steps_per_second': 5.318, 'total_flos': 7613745476812800.0, 'train_loss': 0.8132031356811523, 'epoch': 800.0})"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\"\"\"\n",
        "TRAINING\n",
        "\"\"\"\n",
        "\n",
        "from transformers import DataCollatorForSeq2Seq, Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
        "\n",
        "dataset = {'train': CustomDataset(data['train'], tokenizer),\n",
        "           'dev': CustomDataset(data['dev'], tokenizer)}\n",
        "data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model)\n",
        "\n",
        "training_args = Seq2SeqTrainingArguments(\n",
        "    max_steps=20000,\n",
        "    per_device_train_batch_size=400,\n",
        "    learning_rate = 0.001,\n",
        "    lr_scheduler_type='inverse_sqrt',\n",
        "    warmup_steps=4000,\n",
        "    adam_beta2=0.98,\n",
        "    label_smoothing_factor=0.1,\n",
        "    evaluation_strategy=\"steps\",\n",
        "    eval_steps=400,\n",
        "    eval_delay=400,\n",
        "    save_strategy=\"steps\",\n",
        "    save_steps=400,\n",
        "    predict_with_generate=True,\n",
        "    metric_for_best_model='exact_match',\n",
        "    save_total_limit=1,\n",
        "    logging_strategy=\"steps\",\n",
        "    logging_steps=400,\n",
        "    output_dir='baseline_0.2',\n",
        "    overwrite_output_dir=True,\n",
        "    load_best_model_at_end=True\n",
        ")\n",
        "\n",
        "trainer = Seq2SeqTrainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    data_collator=data_collator,\n",
        "    train_dataset=dataset['train'],\n",
        "    eval_dataset=dataset['dev'],\n",
        "    tokenizer=tokenizer,\n",
        "    compute_metrics=compute_metrics\n",
        ")\n",
        "\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MZCTkEBfH__-"
      },
      "outputs": [],
      "source": [
        "###DO NOT CHANGE THIS CELL###\n",
        "\"\"\"\n",
        "EVALUATION\n",
        "\"\"\"\n",
        "\n",
        "test_data_path = 'https://raw.githubusercontent.com/sigmorphon/2023InflectionST/main/part1/data/nav.tst'\n",
        "test_data = download_data(test_data_path)\n",
        "test_dataset = CustomDataset(preprocess_data(test_data), tokenizer)\n",
        "result = trainer.evaluate(test_dataset)\n",
        "test_accuracy_wu = result['eval_exact_match']\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S1Nbr53pLIs2"
      },
      "source": [
        "### Test Accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "20PvBNLOLIs2",
        "outputId": "befec8ff-6d79-4acb-e06c-f722abebb6fe"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test accuracy: 0.488\n"
          ]
        }
      ],
      "source": [
        "###DO NOT CHANGE THIS CELL###\n",
        "print('Test accuracy:', test_accuracy_wu)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lx31P1GOLIs3"
      },
      "source": [
        "### Report\n",
        "[Describe your approach in up to 300 words here.]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UQ9QM7u3LIs3"
      },
      "source": [
        "# Task 2\n",
        "Build a better morphological inflector."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f04Vp_VtLIs3"
      },
      "outputs": [],
      "source": [
        "# your code goes here - make sure the final test accuracy is saved as test_accuracy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZjcrL7XHLIs3"
      },
      "source": [
        "### Test Accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bRYYuS8LLIs4"
      },
      "outputs": [],
      "source": [
        "###DO NOT CHANGE THIS CELL###\n",
        "print('Test accuracy:', test_accuracy)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qr9oE9wGLIs4"
      },
      "source": [
        "### Report\n",
        "[Describe your approach in up to 300 words here.]"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}