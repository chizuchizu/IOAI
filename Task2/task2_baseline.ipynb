{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/chizuchizu/IOAI/blob/main/Task2/task2_baseline.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dEDVuanhJGuV"
      },
      "outputs": [],
      "source": [
        "# ====================================================\n",
        "# CFG\n",
        "# ====================================================\n",
        "\n",
        "class CFG:\n",
        "    num_workers=4\n",
        "    project = \"\"\n",
        "    name = \"\"\n",
        "\n",
        "    # model\n",
        "    base_model_name = \"\"\n",
        "    tokenizer_name = \"\"\n",
        "    num_classes = 5\n",
        "\n",
        "    # training\n",
        "    epochs = 20\n",
        "\n",
        "    scheduler='linear' # ['ReduceLROnPlateau', 'CosineAnnealingLR', 'CosineAnnealingWarmRestarts']\n",
        "\n",
        "    lr = 1e-05\n",
        "\n",
        "    # dataset\n",
        "    max_length = 256\n",
        "\n",
        "    # T4: 32\n",
        "    # L4: 64\n",
        "    train_batch_size = 64\n",
        "    eval_batch_size = 64\n",
        "\n",
        "    seed=42\n",
        "    train=True\n",
        "\n",
        "# for wandb\n",
        "cfg = dict(vars(CFG))\n",
        "cfg = {k: v for k, v in cfg.items() if \"__\" not in k}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sV85hgL0yxn0"
      },
      "outputs": [],
      "source": [
        "from google.colab import userdata\n",
        "\n",
        "read_access_token = userdata.get('hf_read')\n",
        "write_access_token = userdata.get('hf_write')\n",
        "\n",
        "import importlib\n",
        "import torch, transformers\n",
        "\n",
        "if '2.3.0' not in torch.__version__:\n",
        "  !pip install torch==2.3.0\n",
        "if transformers.__version__!='4.41.2':\n",
        "  !pip install transformers==4.41.2\n",
        "\n",
        "if importlib.util.find_spec('datasets') is None:\n",
        "  !pip install datasets==2.18.0s\n",
        "  !pip install evaluate==0.4.2\n",
        "  !pip install accelerate -U\n",
        "\n",
        "if importlib.util.find_spec('wandb') is None:\n",
        "  !pip install wandb -q\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.functional import F\n",
        "\n",
        "import torchvision\n",
        "from torchvision import datasets, transforms, models\n",
        "\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "from transformers import AutoModelForSequenceClassification, AutoTokenizer, DataCollatorWithPadding, get_scheduler\n",
        "from datasets import load_dataset, Dataset, DatasetDict\n",
        "\n",
        "import evaluate\n",
        "\n",
        "import wandb\n",
        "\n",
        "from huggingface_hub import login\n",
        "\n",
        "wandb.login(key=userdata.get('wandb_token'))\n",
        "login(token=read_access_token)\n",
        "\n",
        "brahmi_to_devanagari = {\n",
        "    'ğ‘€“': 'à¤•', 'ğ‘€”': 'à¤–', 'ğ‘€•': 'à¤—', 'ğ‘€–': 'à¤˜', 'ğ‘€—': 'à¤™', 'ğ‘€˜': 'à¤š', 'ğ‘€™': 'à¤›',\n",
        "    'ğ‘€š': 'à¤œ', 'ğ‘€›': 'à¤', 'ğ‘€œ': 'à¤', 'ğ‘€': 'à¤Ÿ', 'ğ‘€': 'à¤ ', 'ğ‘€Ÿ': 'à¤¡', 'ğ‘€ ': 'à¤¢',\n",
        "    'ğ‘€¡': 'à¤£', 'ğ‘€¢': 'à¤¤', 'ğ‘€£': 'à¤¥', 'ğ‘€¤': 'à¤¦', 'ğ‘€¥': 'à¤§', 'ğ‘€¦': 'à¤¨', 'ğ‘€§': 'à¤ª',\n",
        "    'ğ‘€¨': 'à¤«', 'ğ‘€©': 'à¤¬', 'ğ‘€ª': 'à¤­', 'ğ‘€«': 'à¤®', 'ğ‘€¬': 'à¤¯', 'ğ‘€­': 'à¤°', 'ğ‘€®': 'à¤²',\n",
        "    'ğ‘€¯': 'à¤µ', 'ğ‘€°': 'à¤¶', 'ğ‘€±': 'à¤·', 'ğ‘€²': 'à¤¸', 'ğ‘€³': 'à¤¹', 'ğ‘¦':'à¥¦', 'ğ‘£': '90'\n",
        "}\n",
        "\n",
        "def transliterate_brahmi_to_devanagari(text):\n",
        "    transliterated_text = ''\n",
        "    for char in text:\n",
        "        if char in brahmi_to_devanagari:\n",
        "            transliterated_text += brahmi_to_devanagari[char]\n",
        "        else:\n",
        "            transliterated_text += char\n",
        "    return transliterated_text\n",
        "\n",
        "f1 = evaluate.load(\"f1\")\n",
        "\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    predictions, labels = eval_pred\n",
        "    predictions = np.argmax(predictions, axis=1)\n",
        "    return f1.compute(predictions=predictions, references=labels, average='macro')\n",
        "\n",
        "def to_device(batch, device):\n",
        "    output = {}\n",
        "    for k, v in batch.items():\n",
        "        try:\n",
        "            output[k] = v.to(device)\n",
        "        except:\n",
        "            output[k] = v\n",
        "    return output\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cHRDuKNBj5IW"
      },
      "outputs": [],
      "source": [
        "def main():\n",
        "\n",
        "    wandb.init(\n",
        "        name=CFG.name,\n",
        "        project=CFG.project,\n",
        "        config=cfg\n",
        "    )\n",
        "\n",
        "    model = AutoModelForSequenceClassification.from_pretrained(\n",
        "        CFG.base_model_name, num_labels=CFG.num_classes\n",
        "    ).cuda()\n",
        "\n",
        "    classification_dataset = load_dataset('InternationalOlympiadAI/NLP_problem', token=read_access_token)\n",
        "    tokenizer = AutoTokenizer.from_pretrained(CFG.tokenizer_name)\n",
        "\n",
        "    def transform(example_batch):\n",
        "        example_batch[\"text\"] = [transliterate_brahmi_to_devanagari(x) for x in example_batch[\"text\"]]\n",
        "        inputs = tokenizer([x for x in example_batch[\"text\"]],  truncation=True, max_length=CFG.max_length, padding=\"max_length\", return_tensors=\"pt\")\n",
        "        inputs[\"labels\"] = example_batch[\"label\"]\n",
        "        return inputs\n",
        "\n",
        "    tokenized_data = classification_dataset.with_transform(transform)\n",
        "\n",
        "    train_dataset = tokenized_data[\"train\"]\n",
        "    eval_dataset = tokenized_data[\"dev\"]\n",
        "\n",
        "    train_loader = DataLoader(\n",
        "        train_dataset,\n",
        "        batch_size=CFG.train_batch_size,\n",
        "        num_workers=0,\n",
        "        pin_memory=True,\n",
        "        shuffle=True,\n",
        "        drop_last=True,\n",
        "    )\n",
        "\n",
        "    eval_loader = DataLoader(\n",
        "        eval_dataset,\n",
        "        batch_size=CFG.eval_batch_size,\n",
        "        num_workers=0,\n",
        "        pin_memory=True,\n",
        "        shuffle=False,\n",
        "    )\n",
        "\n",
        "    num_training_steps = CFG.epochs * len(train_loader)\n",
        "    optimizer = optim.AdamW(model.parameters(), lr=CFG.lr)\n",
        "    scheduler = get_scheduler(name=CFG.scheduler, optimizer=optimizer, num_warmup_steps=0, num_training_steps=num_training_steps)\n",
        "\n",
        "    device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "\n",
        "    def train_one_epoch(model, scheduler, train_loader, optimizer):\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "        progress_bar = tqdm(train_loader, dynamic_ncols=True)\n",
        "\n",
        "        for step, batch in enumerate(progress_bar):\n",
        "            batch = to_device(batch, device)\n",
        "            outputs = model(\n",
        "                input_ids=batch[\"input_ids\"],\n",
        "                attention_mask=batch[\"attention_mask\"],\n",
        "                token_type_ids=batch[\"token_type_ids\"],\n",
        "                labels=batch[\"labels\"],\n",
        "            )\n",
        "            loss = outputs.loss\n",
        "            loss.backward()\n",
        "\n",
        "            text = f\"step {step}, loss: {loss:.5f}\"\n",
        "            progress_bar.set_description(text)\n",
        "\n",
        "            wandb.log(\n",
        "                {\n",
        "                    \"train_loss\": loss,\n",
        "                    \"lr\": optimizer.param_groups[0][\"lr\"],\n",
        "                    \"step\": step,\n",
        "                }\n",
        "            )\n",
        "            optimizer.step()\n",
        "            scheduler.step()\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "    def evaluate_model(model, test_loader):\n",
        "        model.eval()\n",
        "        predictions = []\n",
        "        labels = []\n",
        "        for batch in eval_loader:\n",
        "            batch = to_device(batch, device)\n",
        "            with torch.no_grad():\n",
        "                outputs = model(\n",
        "                    input_ids=batch[\"input_ids\"],\n",
        "                    attention_mask=batch[\"attention_mask\"],\n",
        "                    token_type_ids=batch[\"token_type_ids\"],\n",
        "                    labels=batch[\"labels\"],\n",
        "                )\n",
        "\n",
        "            logits = outputs.logits\n",
        "            prediction = torch.argmax(logits, dim=-1)\n",
        "            predictions.append(prediction.cpu().numpy())\n",
        "            labels.append(batch[\"labels\"].cpu().numpy())\n",
        "\n",
        "        predictions = np.concatenate(predictions)\n",
        "        labels = np.concatenate(labels)\n",
        "        return f1.compute(predictions=predictions, references=labels, average='macro')\n",
        "\n",
        "    # Train and evaluate the model\n",
        "    model.to(device)\n",
        "    for i in range(CFG.epochs):\n",
        "        # accuracy = evaluate_model(model, eval_loader)\n",
        "        train_one_epoch(model, scheduler, train_loader, optimizer)\n",
        "        accuracy = evaluate_model(model, eval_loader)\n",
        "        wandb.log(\n",
        "            {\n",
        "                \"epoch\": i+1,\n",
        "                \"accuracy\": accuracy\n",
        "            }\n",
        "        )\n",
        "        print(f'Epoch {i+1} {accuracy}')\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zs02eqO9jQBx"
      },
      "outputs": [],
      "source": [
        "model = main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lMNqHsF0olYA"
      },
      "outputs": [],
      "source": [
        "model.push_to_hub(\n",
        "    f\"ioai2024japan/{CFG.name}\",\n",
        "    token=userdata.get('hf_write'), private=True\n",
        ")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "bqAFpqJlnt77"
      ],
      "gpuType": "L4",
      "machine_shape": "hm",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}