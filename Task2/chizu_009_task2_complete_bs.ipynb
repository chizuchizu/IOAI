{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/chizuchizu/IOAI/blob/main/Task2/chizu_009_task2_complete_bs.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "dEDVuanhJGuV"
      },
      "outputs": [],
      "source": [
        "# ====================================================\n",
        "# CFG\n",
        "# ====================================================\n",
        "\n",
        "class CFG:\n",
        "    num_workers=4\n",
        "    project = \"IOAI_Task2_classification\"\n",
        "    name = \"chizu_009_task2_complete_bs\"\n",
        "\n",
        "    # pseudo_base_model_name = \"ioai2024japan/redrock_015_task2_finetune\"\n",
        "    base_model_name = \"google-bert/bert-base-multilingual-uncased\"\n",
        "    base_tokenizer_name = \"google-bert/bert-base-multilingual-uncased\"\n",
        "\n",
        "    # None -> training, otherwise -> load the model\n",
        "    # pretrained_model_name = None # \"ioai2024japan/redrock_005_task2_pretrain_wandb\"\n",
        "    pretrained_model_name = None # \"/content/chizu_004_task2_complete_5e-5_pretrain\"\n",
        "    # tokenizer_name = \"google-bert/bert-base-multilingual-uncased\"\n",
        "    num_classes = 5\n",
        "\n",
        "    # training\n",
        "    pretrain_epochs = 2\n",
        "    classification_epochs = 30\n",
        "    mlm_probability = 0.15\n",
        "\n",
        "    scheduler='linear' # ['ReduceLROnPlateau', 'CosineAnnealingLR', 'CosineAnnealingWarmRestarts']\n",
        "\n",
        "    lr = 5e-5\n",
        "\n",
        "    # dataset\n",
        "    max_length = 256\n",
        "\n",
        "    train_batch_size = 64\n",
        "    eval_batch_size = 64\n",
        "\n",
        "    seed=42\n",
        "    train=True\n",
        "\n",
        "    pseudo_size = 60000\n",
        "    pseudo_select_size = 1500\n",
        "\n",
        "    if_wandb = True\n",
        "\n",
        "# for wandb\n",
        "cfg = dict(vars(CFG))\n",
        "cfg = {k: v for k, v in cfg.items() if \"__\" not in k}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sV85hgL0yxn0"
      },
      "outputs": [],
      "source": [
        "from google.colab import userdata\n",
        "\n",
        "read_access_token = userdata.get('hf_read')\n",
        "write_access_token = userdata.get('hf_write')\n",
        "\n",
        "import importlib\n",
        "import torch, transformers\n",
        "\n",
        "if '2.3.0' not in torch.__version__:\n",
        "  !pip install torch==2.3.0\n",
        "if transformers.__version__!='4.41.2':\n",
        "  !pip install transformers==4.41.2\n",
        "\n",
        "if importlib.util.find_spec('datasets') is None:\n",
        "  !pip install datasets==2.18.0\n",
        "  !pip install evaluate==0.4.2\n",
        "  !pip install accelerate -U\n",
        "\n",
        "if importlib.util.find_spec('wandb') is None:\n",
        "  !pip install wandb -q\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.functional import F\n",
        "import torch.cuda.amp as amp\n",
        "\n",
        "import torchvision\n",
        "from torchvision import datasets, transforms, models\n",
        "\n",
        "from tqdm.auto import tqdm\n",
        "from transformers import DataCollatorForLanguageModeling\n",
        "\n",
        "from transformers import AutoModelForSequenceClassification, AutoTokenizer, DataCollatorWithPadding, get_scheduler, BertForMaskedLM, BertTokenizer\n",
        "\n",
        "from datasets import load_dataset, Dataset, DatasetDict, concatenate_datasets\n",
        "\n",
        "import evaluate\n",
        "\n",
        "import wandb\n",
        "\n",
        "from joblib import Parallel, delayed\n",
        "\n",
        "from huggingface_hub import login\n",
        "\n",
        "wandb.login(key=userdata.get('wandb_token'))\n",
        "login(token=read_access_token)\n",
        "\n",
        "brahmi_to_devanagari = {\n",
        "    'ð‘€“': 'à¤•', 'ð‘€”': 'à¤–', 'ð‘€•': 'à¤—', 'ð‘€–': 'à¤˜', 'ð‘€—': 'à¤™', 'ð‘€˜': 'à¤š', 'ð‘€™': 'à¤›',\n",
        "    'ð‘€š': 'à¤œ', 'ð‘€›': 'à¤', 'ð‘€œ': 'à¤ž', 'ð‘€': 'à¤Ÿ', 'ð‘€ž': 'à¤ ', 'ð‘€Ÿ': 'à¤¡', 'ð‘€ ': 'à¤¢',\n",
        "    'ð‘€¡': 'à¤£', 'ð‘€¢': 'à¤¤', 'ð‘€£': 'à¤¥', 'ð‘€¤': 'à¤¦', 'ð‘€¥': 'à¤§', 'ð‘€¦': 'à¤¨', 'ð‘€§': 'à¤ª',\n",
        "    'ð‘€¨': 'à¤«', 'ð‘€©': 'à¤¬', 'ð‘€ª': 'à¤­', 'ð‘€«': 'à¤®', 'ð‘€¬': 'à¤¯', 'ð‘€­': 'à¤°', 'ð‘€®': 'à¤²',\n",
        "    'ð‘€¯': 'à¤µ', 'ð‘€°': 'à¤¶', 'ð‘€±': 'à¤·', 'ð‘€²': 'à¤¸', 'ð‘€³': 'à¤¹', 'ð‘¦':'à¥¦', 'ð‘£': '90'\n",
        "}\n",
        "\n",
        "def transliterate_brahmi_to_devanagari(text):\n",
        "    transliterated_text = ''\n",
        "    for char in text:\n",
        "        if char in brahmi_to_devanagari:\n",
        "            transliterated_text += brahmi_to_devanagari[char]\n",
        "        else:\n",
        "            transliterated_text += char\n",
        "    return transliterated_text\n",
        "\n",
        "f1 = evaluate.load(\"f1\")\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    predictions, labels = eval_pred\n",
        "    predictions = np.argmax(predictions, axis=1)\n",
        "    return f1.compute(predictions=predictions, references=labels, average='macro')\n",
        "\n",
        "def to_device(batch, device):\n",
        "    output = {}\n",
        "    for k, v in batch.items():\n",
        "        try:\n",
        "            output[k] = v.to(device)\n",
        "        except:\n",
        "            output[k] = v\n",
        "    return output\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MDkb_0oWkPcn"
      },
      "outputs": [],
      "source": [
        "def train_tokenizer(raw_dataset):\n",
        "    train_corpus = []\n",
        "\n",
        "    num_cores = 8\n",
        "\n",
        "    train_corpus = Parallel(n_jobs=num_cores)(\n",
        "        delayed(transliterate_brahmi_to_devanagari)(text) for text in tqdm(raw_dataset['train'][\"text\"])\n",
        "    )\n",
        "\n",
        "    base_tokenizer = AutoTokenizer.from_pretrained(CFG.base_tokenizer_name)\n",
        "\n",
        "    tokenizer = base_tokenizer.train_new_from_iterator(train_corpus, base_tokenizer.vocab_size)\n",
        "\n",
        "    return tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WLX-zdDfdTXy"
      },
      "outputs": [],
      "source": [
        "def train_one_epoch(model, scheduler, train_loader, optimizer, fp16=False):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    progress_bar = tqdm(train_loader, dynamic_ncols=True)\n",
        "    scaler = torch.cuda.amp.GradScaler()\n",
        "\n",
        "    for step, batch in enumerate(progress_bar):\n",
        "        batch = to_device(batch, \"cuda\")\n",
        "\n",
        "        if fp16:\n",
        "            with amp.autocast():\n",
        "                outputs = model(\n",
        "                    input_ids=batch[\"input_ids\"],\n",
        "                    attention_mask=batch[\"attention_mask\"],\n",
        "                    token_type_ids=batch[\"token_type_ids\"],\n",
        "                    labels=batch[\"labels\"],\n",
        "                )\n",
        "                loss = outputs.loss\n",
        "\n",
        "            # Scale loss for fp16 training\n",
        "            scaler.scale(loss).backward()\n",
        "\n",
        "            # Optimizer step with gradient scaling\n",
        "            scaler.step(optimizer)\n",
        "            scaler.update()\n",
        "            optimizer.zero_grad()\n",
        "            scheduler.step()\n",
        "        else:\n",
        "            outputs = model(\n",
        "                input_ids=batch[\"input_ids\"],\n",
        "                attention_mask=batch[\"attention_mask\"],\n",
        "                token_type_ids=batch[\"token_type_ids\"],\n",
        "                labels=batch[\"labels\"],\n",
        "            )\n",
        "            loss = outputs.loss\n",
        "            loss.backward()\n",
        "\n",
        "            optimizer.step()\n",
        "            scheduler.step()\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "        if CFG.if_wandb:\n",
        "            wandb.log(\n",
        "                {\n",
        "                    \"train_loss\": loss,\n",
        "                    \"lr\": optimizer.param_groups[0][\"lr\"],\n",
        "                    \"step\": step,\n",
        "                }\n",
        "            )\n",
        "\n",
        "        text = f\"step {step}, loss: {loss:.5f}\"\n",
        "        progress_bar.set_description(text)\n",
        "\n",
        "def evaluate_model(model, eval_loader):\n",
        "    model.eval()\n",
        "    predictions = []\n",
        "    labels = []\n",
        "    for batch in eval_loader:\n",
        "        batch = to_device(batch, \"cuda\")\n",
        "        with torch.no_grad():\n",
        "            outputs = model(\n",
        "                input_ids=batch[\"input_ids\"],\n",
        "                attention_mask=batch[\"attention_mask\"],\n",
        "                token_type_ids=batch[\"token_type_ids\"],\n",
        "                labels=batch[\"labels\"],\n",
        "            )\n",
        "\n",
        "        logits = outputs.logits\n",
        "        prediction = torch.argmax(logits, dim=-1)\n",
        "        predictions.append(prediction.cpu().numpy())\n",
        "        labels.append(batch[\"labels\"].cpu().numpy())\n",
        "\n",
        "    predictions = np.concatenate(predictions)\n",
        "    labels = np.concatenate(labels)\n",
        "    f1_score = f1.compute(predictions=predictions, references=labels, average='macro')\n",
        "    return f1_score, predictions\n",
        "\n",
        "def test_model(model, eval_loader):\n",
        "    model.eval()\n",
        "    predictions = []\n",
        "    labels = []\n",
        "    for batch in eval_loader:\n",
        "        batch = to_device(batch, \"cuda\")\n",
        "        with torch.no_grad():\n",
        "            outputs = model(\n",
        "                input_ids=batch[\"input_ids\"],\n",
        "                attention_mask=batch[\"attention_mask\"],\n",
        "                token_type_ids=batch[\"token_type_ids\"],\n",
        "            )\n",
        "\n",
        "        logits = outputs.logits\n",
        "        prediction = torch.argmax(logits, dim=-1)\n",
        "        predictions.append(prediction.cpu().numpy())\n",
        "\n",
        "    predictions = np.concatenate(predictions)\n",
        "    return predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1SSBvno3jCGv"
      },
      "outputs": [],
      "source": [
        "def pretrain(raw_dataset, tokenizer, transform_raw, fp16=True):\n",
        "    print(\"=== Pretrain ===\")\n",
        "\n",
        "    data_collator = DataCollatorForLanguageModeling(\n",
        "        tokenizer=tokenizer,\n",
        "        mlm=True,\n",
        "        mlm_probability=CFG.mlm_probability\n",
        "    )\n",
        "\n",
        "    tokenized_data = raw_dataset.with_transform(transform_raw)\n",
        "\n",
        "    train_dataset = tokenized_data[\"train\"]\n",
        "\n",
        "    train_loader = DataLoader(\n",
        "        train_dataset,\n",
        "        batch_size=CFG.train_batch_size,\n",
        "        num_workers=0,\n",
        "        pin_memory=True,\n",
        "        shuffle=True,\n",
        "        drop_last=True,\n",
        "        collate_fn=data_collator,\n",
        "    )\n",
        "\n",
        "    model = BertForMaskedLM.from_pretrained(\n",
        "        CFG.base_model_name\n",
        "    ).cuda()\n",
        "\n",
        "    num_training_steps = CFG.pretrain_epochs * len(train_loader)\n",
        "    optimizer = optim.AdamW(model.parameters(), lr=CFG.lr)\n",
        "    scheduler = get_scheduler(name=CFG.scheduler, optimizer=optimizer, num_warmup_steps=0, num_training_steps=num_training_steps)\n",
        "\n",
        "    for i in range(CFG.pretrain_epochs):\n",
        "        train_one_epoch(model, scheduler, train_loader, optimizer, fp16)\n",
        "        print(f'Epoch {i+1}')\n",
        "\n",
        "    model_path = f\"{CFG.name}_pretrain\"\n",
        "    model.save_pretrained(model_path)\n",
        "    return model_path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jXAjWlZFmBlm"
      },
      "outputs": [],
      "source": [
        "def finetine(base_model, train_dataset, eval_dataset, device, fp16=False):\n",
        "    print(\"=== Finetune ===\")\n",
        "    model = AutoModelForSequenceClassification.from_pretrained(\n",
        "        base_model, num_labels=CFG.num_classes\n",
        "    ).cuda()\n",
        "\n",
        "    train_loader = DataLoader(\n",
        "        train_dataset,\n",
        "        batch_size=CFG.train_batch_size,\n",
        "        num_workers=0,\n",
        "        pin_memory=True,\n",
        "        shuffle=True,\n",
        "        drop_last=True,\n",
        "    )\n",
        "\n",
        "    eval_loader = DataLoader(\n",
        "        eval_dataset,\n",
        "        batch_size=CFG.eval_batch_size,\n",
        "        num_workers=0,\n",
        "        pin_memory=True,\n",
        "        shuffle=False,\n",
        "        drop_last=False,\n",
        "    )\n",
        "\n",
        "    num_training_steps = CFG.classification_epochs * len(train_loader)\n",
        "    optimizer = optim.AdamW(model.parameters(), lr=CFG.lr)\n",
        "    scheduler = get_scheduler(name=CFG.scheduler, optimizer=optimizer, num_warmup_steps=0, num_training_steps=num_training_steps)\n",
        "\n",
        "    best_f1 = 0.0\n",
        "    best_model_path = None\n",
        "\n",
        "    model.to(device)\n",
        "    for i in range(CFG.classification_epochs):\n",
        "        train_one_epoch(model, scheduler, train_loader, optimizer)\n",
        "        f1_score, _ = evaluate_model(model, eval_loader)\n",
        "        f1_score = f1_score[\"f1\"]\n",
        "\n",
        "        if f1_score > best_f1:\n",
        "            best_f1 = f1_score\n",
        "            best_model_path = f\"{CFG.name}_finetune_epoch_{i+1}\"\n",
        "            model.save_pretrained(best_model_path)\n",
        "\n",
        "\n",
        "        if CFG.if_wandb:\n",
        "            wandb.log(\n",
        "                {\n",
        "                    \"epoch\": i+1,\n",
        "                    \"f1\": f1_score\n",
        "                }\n",
        "            )\n",
        "        print(f'Epoch {i+1} {f1_score}')\n",
        "\n",
        "    # model_path = f\"{CFG.name}_finetune\"\n",
        "    # model.save_pretrained(model_path)\n",
        "    return best_model_path"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def up_to_hub(model_name, model_path, tokenizer):\n",
        "    model = AutoModelForSequenceClassification.from_pretrained(\n",
        "        model_path, num_labels=CFG.num_classes\n",
        "    )\n",
        "    model.push_to_hub(\n",
        "        f\"ioai2024japan/{model_name}\",\n",
        "        token=write_access_token, private=True\n",
        "    )\n",
        "    tokenizer.push_to_hub(\n",
        "        f\"ioai2024japan/{model_name}\",\n",
        "        token=write_access_token, private=True\n",
        "    )"
      ],
      "metadata": {
        "id": "3P_oA7-FPATK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cHRDuKNBj5IW"
      },
      "outputs": [],
      "source": [
        "def main():\n",
        "\n",
        "\n",
        "\n",
        "    raw_dataset = load_dataset('InternationalOlympiadAI/NLP_problem_raw', token=read_access_token)\n",
        "    classification_dataset = load_dataset('InternationalOlympiadAI/NLP_problem', token=read_access_token)\n",
        "\n",
        "    device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "\n",
        "    # Train tokenizer to Lang X\n",
        "    tokenizer = train_tokenizer(raw_dataset)\n",
        "\n",
        "    # for raw set\n",
        "    def transform_raw(example_batch):\n",
        "        example_batch[\"text\"] = [transliterate_brahmi_to_devanagari(x) for x in example_batch[\"text\"]]\n",
        "        inputs = tokenizer([x for x in example_batch[\"text\"]],  truncation=True, max_length=CFG.max_length, padding=\"max_length\", return_tensors=\"pt\")\n",
        "        return inputs\n",
        "\n",
        "    # for problem set\n",
        "    def transform(example_batch):\n",
        "        example_batch[\"text\"] = [transliterate_brahmi_to_devanagari(x) for x in example_batch[\"text\"]]\n",
        "        inputs = tokenizer([x for x in example_batch[\"text\"]],  truncation=True, max_length=CFG.max_length, padding=\"max_length\", return_tensors=\"pt\")\n",
        "        inputs[\"labels\"] = example_batch[\"label\"]\n",
        "        return inputs\n",
        "\n",
        "    tokenized_data = classification_dataset.with_transform(transform)\n",
        "\n",
        "    train_dataset = tokenized_data[\"train\"]\n",
        "    eval_dataset = tokenized_data[\"dev\"]\n",
        "\n",
        "    # Continual Pre-Training of MLM\n",
        "    if CFG.pretrained_model_name is None:\n",
        "        if CFG.if_wandb:\n",
        "            wandb.init(\n",
        "                name=CFG.name,\n",
        "                project=\"IOAI_Task2_pretrain\",\n",
        "                config=cfg\n",
        "            )\n",
        "        pretrained_model_path = pretrain(raw_dataset, tokenizer, transform_raw, fp16=True)\n",
        "        up_to_hub(f\"{CFG.name}_pretrain\", pretrained_model_path, tokenizer)\n",
        "    else:\n",
        "        pretrained_model_path = CFG.pretrained_model_name\n",
        "\n",
        "    if CFG.if_wandb:\n",
        "        wandb.init(\n",
        "            name=CFG.name,\n",
        "            project=\"IOAI_Task2_finetune\",\n",
        "            config=cfg\n",
        "        )\n",
        "\n",
        "    # Finetune with normal dataset\n",
        "    finetuned_model_path = finetine(pretrained_model_path, train_dataset, eval_dataset, device)\n",
        "\n",
        "    if CFG.if_wandb:\n",
        "        wandb.finish()\n",
        "\n",
        "    # # Get pseudo label\n",
        "    # pseudo_data, confidences = pseudo_get_data(raw_dataset, transform_raw, finetuned_model_path, device)\n",
        "    # pseudo_labeled_tokens = pseudo_data.with_transform(transform)\n",
        "\n",
        "    # combined_train_dataset = concatenate_datasets([pseudo_labeled_tokens, train_dataset])\n",
        "\n",
        "    # # Finetune with pseudo dataset and normal dataset\n",
        "    # final_model_name = finetine(pretrained_model_path, combined_train_dataset, eval_dataset, device)\n",
        "\n",
        "    return finetuned_model_path, tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zs02eqO9jQBx"
      },
      "outputs": [],
      "source": [
        "final_model_name, tokenizer = main()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "up_to_hub(CFG.name, final_model_name, tokenizer)"
      ],
      "metadata": {
        "id": "PJhxgL4TuU9F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lMNqHsF0olYA"
      },
      "outputs": [],
      "source": [
        "model = AutoModelForSequenceClassification.from_pretrained(\n",
        "    final_model_name, num_labels=CFG.num_classes\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# run the trained model on a dev/test split\n",
        "classification_dataset = load_dataset('InternationalOlympiadAI/NLP_problem', token=read_access_token)\n",
        "\n",
        "def transform_raw(example_batch):\n",
        "    example_batch[\"text\"] = [transliterate_brahmi_to_devanagari(x) for x in example_batch[\"text\"]]\n",
        "    inputs = tokenizer([x for x in example_batch[\"text\"]],  truncation=True, max_length=CFG.max_length, padding=\"max_length\", return_tensors=\"pt\")\n",
        "    return inputs\n",
        "\n",
        "data_split = \"dev\"\n",
        "tokenized_data = classification_dataset.with_transform(transform_raw)\n",
        "test_dataset = tokenized_data[data_split]\n",
        "\n",
        "test_loader = DataLoader(\n",
        "    test_dataset,\n",
        "    batch_size=CFG.eval_batch_size,\n",
        "    num_workers=0,\n",
        "    pin_memory=True,\n",
        "    shuffle=False,\n",
        "    drop_last=False,\n",
        ")\n",
        "model.cuda()"
      ],
      "metadata": {
        "id": "Gg3XyWrXR4u0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predictions = test_model(model, test_loader)"
      ],
      "metadata": {
        "id": "ro1IVGKtfyvp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# write the predictions to a file\n",
        "with open('{}_predictions.txt'.format(data_split), 'w') as outfile:\n",
        "  outfile.write('\\n'.join([str(p) for p in predictions.tolist()]))"
      ],
      "metadata": {
        "id": "s9npUvQ3R7cr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predictions"
      ],
      "metadata": {
        "id": "RFe6sNrcgJa_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-sToEYVZ3dmT"
      },
      "outputs": [],
      "source": [
        "def terminate_session():\n",
        "    # Terminate this session\n",
        "\n",
        "    from google.colab import runtime\n",
        "    runtime.unassign()\n",
        "\n",
        "terminate_session()"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "n4JJkmOqfj5Q"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "machine_shape": "hm",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}