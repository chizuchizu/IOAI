{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/chizuchizu/IOAI/blob/main/redrock_000_task2_complete.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dEDVuanhJGuV"
      },
      "outputs": [],
      "source": [
        "# ====================================================\n",
        "# CFG\n",
        "# ====================================================\n",
        "\n",
        "class CFG:\n",
        "    num_workers=4\n",
        "    project = \"IOAI_Task2_classification\"\n",
        "    name = \"redrock_000_task2_complete\"\n",
        "\n",
        "    # pseudo_base_model_name = \"ioai2024japan/redrock_015_task2_finetune\"\n",
        "    base_model_name = \"google-bert/bert-base-multilingual-uncased\"\n",
        "    base_tokenizer_name = \"google-bert/bert-base-multilingual-uncased\"\n",
        "    # tokenizer_name = \"google-bert/bert-base-multilingual-uncased\"\n",
        "    num_classes = 5\n",
        "\n",
        "    # training\n",
        "    pretrain_epochs = 1\n",
        "    classification_epochs = 30\n",
        "\n",
        "    scheduler='linear' # ['ReduceLROnPlateau', 'CosineAnnealingLR', 'CosineAnnealingWarmRestarts']\n",
        "\n",
        "    lr = 1e-05\n",
        "\n",
        "    # dataset\n",
        "    max_length = 256\n",
        "\n",
        "    # T4: 32\n",
        "    # L4: 64\n",
        "    train_batch_size = 32\n",
        "    eval_batch_size = 32\n",
        "\n",
        "    seed=42\n",
        "    train=True\n",
        "\n",
        "    pseudo_size = 60000\n",
        "    pseudo_select_size = 1500\n",
        "\n",
        "    if_wandb = False\n",
        "\n",
        "# for wandb\n",
        "cfg = dict(vars(CFG))\n",
        "cfg = {k: v for k, v in cfg.items() if \"__\" not in k}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sV85hgL0yxn0"
      },
      "outputs": [],
      "source": [
        "from google.colab import userdata\n",
        "\n",
        "read_access_token = userdata.get('hf_read')\n",
        "write_access_token = userdata.get('hf_write')\n",
        "\n",
        "import importlib\n",
        "import torch, transformers\n",
        "\n",
        "if '2.3.0' not in torch.__version__:\n",
        "  !pip install torch==2.3.0\n",
        "if transformers.__version__!='4.41.2':\n",
        "  !pip install transformers==4.41.2\n",
        "\n",
        "if importlib.util.find_spec('datasets') is None:\n",
        "  !pip install datasets==2.18.0s\n",
        "  !pip install evaluate==0.4.2\n",
        "  !pip install accelerate -U\n",
        "\n",
        "if importlib.util.find_spec('wandb') is None:\n",
        "  !pip install wandb -q\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.functional import F\n",
        "\n",
        "import torchvision\n",
        "from torchvision import datasets, transforms, models\n",
        "\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "from transformers import AutoModelForSequenceClassification, AutoTokenizer, DataCollatorWithPadding, get_scheduler\n",
        "from datasets import load_dataset, Dataset, DatasetDict, concatenate_datasets\n",
        "\n",
        "import evaluate\n",
        "\n",
        "import wandb\n",
        "\n",
        "from joblib import Parallel, delayed\n",
        "\n",
        "from huggingface_hub import login\n",
        "\n",
        "wandb.login(key=userdata.get('wandb_token'))\n",
        "login(token=read_access_token)\n",
        "\n",
        "brahmi_to_devanagari = {\n",
        "    '𑀓': 'क', '𑀔': 'ख', '𑀕': 'ग', '𑀖': 'घ', '𑀗': 'ङ', '𑀘': 'च', '𑀙': 'छ',\n",
        "    '𑀚': 'ज', '𑀛': 'झ', '𑀜': 'ञ', '𑀝': 'ट', '𑀞': 'ठ', '𑀟': 'ड', '𑀠': 'ढ',\n",
        "    '𑀡': 'ण', '𑀢': 'त', '𑀣': 'थ', '𑀤': 'द', '𑀥': 'ध', '𑀦': 'न', '𑀧': 'प',\n",
        "    '𑀨': 'फ', '𑀩': 'ब', '𑀪': 'भ', '𑀫': 'म', '𑀬': 'य', '𑀭': 'र', '𑀮': 'ल',\n",
        "    '𑀯': 'व', '𑀰': 'श', '𑀱': 'ष', '𑀲': 'स', '𑀳': 'ह', '𑁦':'०', '𑁣': '90'\n",
        "}\n",
        "\n",
        "def transliterate_brahmi_to_devanagari(text):\n",
        "    transliterated_text = ''\n",
        "    for char in text:\n",
        "        if char in brahmi_to_devanagari:\n",
        "            transliterated_text += brahmi_to_devanagari[char]\n",
        "        else:\n",
        "            transliterated_text += char\n",
        "    return transliterated_text\n",
        "\n",
        "f1 = evaluate.load(\"f1\")\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    predictions, labels = eval_pred\n",
        "    predictions = np.argmax(predictions, axis=1)\n",
        "    return f1.compute(predictions=predictions, references=labels, average='macro')\n",
        "\n",
        "def to_device(batch, device):\n",
        "    output = {}\n",
        "    for k, v in batch.items():\n",
        "        try:\n",
        "            output[k] = v.to(device)\n",
        "        except:\n",
        "            output[k] = v\n",
        "    return output\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def pred_pseudo(model, train_loader, device):\n",
        "    model.eval()\n",
        "    predictions_list = []\n",
        "    confidences_list = []\n",
        "    for batch in tqdm(train_loader):\n",
        "        batch = to_device(batch, device)\n",
        "        with torch.no_grad():\n",
        "            outputs = model(**batch)\n",
        "\n",
        "        logits = outputs.logits\n",
        "        confidences = F.softmax(logits, dim=-1)\n",
        "        pred_confidence,_ = confidences.max(dim=-1)\n",
        "        #pred_confidence,_ = pred_confidence_temp.max(dim=-1)\n",
        "        # print(pred_confidence.shape)\n",
        "        predictions = torch.argmax(logits, dim=-1)\n",
        "        #predictions = torch.mode(predictions, dim=0).values\n",
        "        # print(predictions.shape)\n",
        "        predictions_list.extend(predictions.cpu().numpy())\n",
        "        confidences_list.extend(pred_confidence.cpu().numpy())\n",
        "\n",
        "    return predictions_list, confidences_list"
      ],
      "metadata": {
        "id": "S46cIsPy2ipO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def pseudo_get_data(raw_dataset, transform_raw, model, device):\n",
        "    pseudo_size = CFG.pseudo_size\n",
        "\n",
        "    raw_eval_batch = raw_dataset[\"train\"].select(range(0, pseudo_size))\n",
        "\n",
        "    print(raw_eval_batch)\n",
        "\n",
        "    tokenized_eval_batch = raw_eval_batch.with_transform(transform_raw)\n",
        "\n",
        "    raw_loader = DataLoader(\n",
        "        tokenized_eval_batch,\n",
        "        batch_size=CFG.train_batch_size,\n",
        "        num_workers=0,\n",
        "        pin_memory=True,\n",
        "        shuffle=True,\n",
        "        drop_last=True,\n",
        "    )\n",
        "\n",
        "    predictions, confidences = pred_pseudo(model, raw_loader, device)\n",
        "\n",
        "    top_conf = np.argsort(confidences)[-CFG.pseudo_select_size:]\n",
        "\n",
        "    selected_texts = [transliterate_brahmi_to_devanagari(raw_eval_batch[int(i)][\"text\"]) for i in top_conf]\n",
        "    selected_labels = [predictions[int(i)] for i in top_conf]\n",
        "\n",
        "    pseudo_labeled_dataset = Dataset.from_dict({\n",
        "        'text': selected_texts,\n",
        "        'label': selected_labels\n",
        "    })\n",
        "\n",
        "    return pseudo_labeled_dataset, confidences"
      ],
      "metadata": {
        "id": "9PcOj9AF2oHJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def transform_raw(tokenizer, example_batch):\n",
        "    example_batch[\"text\"] = [transliterate_brahmi_to_devanagari(x) for x in example_batch[\"text\"]]\n",
        "    inputs = tokenizer([x for x in example_batch[\"text\"]],  truncation=True, max_length=CFG.max_length, padding=\"max_length\", return_tensors=\"pt\")\n",
        "    return inputs\n",
        "\n",
        "def transform(tokenizer, example_batch):\n",
        "    example_batch[\"text\"] = [transliterate_brahmi_to_devanagari(x) for x in example_batch[\"text\"]]\n",
        "    inputs = tokenizer([x for x in example_batch[\"text\"]],  truncation=True, max_length=CFG.max_length, padding=\"max_length\", return_tensors=\"pt\")\n",
        "    inputs[\"labels\"] = example_batch[\"label\"]\n",
        "    return inputs"
      ],
      "metadata": {
        "id": "wx4FgIP4lF_v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_tokenizer(raw_dataset):\n",
        "    train_corpus = []\n",
        "\n",
        "    num_cores = 8\n",
        "\n",
        "    train_corpus = Parallel(n_jobs=num_cores)(\n",
        "        delayed(transliterate_brahmi_to_devanagari)(text) for text in tqdm(raw_dataset['train'][\"text\"])\n",
        "    )\n",
        "\n",
        "    # print(train_corpus[:5])\n",
        "\n",
        "    old_tokenizer = AutoTokenizer.from_pretrained(CFG.base_tokenizer_name)\n",
        "\n",
        "    tokenizer = old_tokenizer.train_new_from_iterator(train_corpus, old_tokenizer.vocab_size)\n",
        "\n",
        "    return tokenizer"
      ],
      "metadata": {
        "id": "MDkb_0oWkPcn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def pretrain(raw_dataset, tokenizer):\n",
        "\n",
        "    data_collator = DataCollatorForLanguageModeling(\n",
        "        tokenizer=tokenizer,\n",
        "        mlm=True,\n",
        "        mlm_probability=CFG.mlm_probability\n",
        "    )\n",
        "\n",
        "    tokenized_data = raw_dataset.with_transform(transform_raw)\n",
        "\n",
        "    train_dataset = tokenized_data[\"train\"]\n",
        "\n",
        "    train_loader = DataLoader(\n",
        "        train_dataset,\n",
        "        batch_size=CFG.train_batch_size,\n",
        "        num_workers=0,\n",
        "        pin_memory=True,\n",
        "        shuffle=True,\n",
        "        drop_last=True,\n",
        "        collate_fn=data_collator,\n",
        "    )\n",
        "\n",
        "    model = BertForMaskedLM.from_pretrained(\n",
        "        CFG.base_model_name\n",
        "    ).cuda()\n",
        "\n",
        "    num_training_steps = CFG.epochs * len(combined_train_loader)\n",
        "    optimizer = optim.AdamW(model.parameters(), lr=CFG.lr)\n",
        "    scheduler = get_scheduler(name=CFG.scheduler, optimizer=optimizer, num_warmup_steps=0, num_training_steps=num_training_steps)\n",
        "\n",
        "    def train_one_epoch(model, scheduler, train_loader, optimizer):\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "        progress_bar = tqdm(train_loader, dynamic_ncols=True)\n",
        "\n",
        "        for step, batch in enumerate(progress_bar):\n",
        "            batch = to_device(batch, device)\n",
        "            outputs = model(\n",
        "                input_ids=batch[\"input_ids\"],\n",
        "                attention_mask=batch[\"attention_mask\"],\n",
        "                token_type_ids=batch[\"token_type_ids\"],\n",
        "                labels=batch[\"labels\"],\n",
        "            )\n",
        "            loss = outputs.loss\n",
        "            loss.backward()\n",
        "\n",
        "            text = f\"step {step}, loss: {loss:.5f}\"\n",
        "            progress_bar.set_description(text)\n",
        "            optimizer.step()\n",
        "            scheduler.step()\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "        for i in range(CFG.pretrain_epochs):\n",
        "            train_one_epoch(model, scheduler, train_loader, optimizer)\n",
        "            print(f'Epoch {i+1}')\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "1SSBvno3jCGv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def fine_tine(base_model, tokenizer, classification_dataset, device):\n",
        "    model = AutoModelForSequenceClassification.from_pretrained(\n",
        "        base_model, num_labels=CFG.num_classes\n",
        "    ).cuda()\n",
        "\n",
        "    tokenized_data = classification_dataset.with_transform(transform)\n",
        "\n",
        "    train_dataset = tokenized_data[\"train\"]\n",
        "    eval_dataset = tokenized_data[\"dev\"]\n",
        "\n",
        "    train_loader = DataLoader(\n",
        "        train_dataset,\n",
        "        batch_size=CFG.train_batch_size,\n",
        "        num_workers=0,\n",
        "        pin_memory=True,\n",
        "        shuffle=True,\n",
        "        drop_last=True,\n",
        "    )\n",
        "\n",
        "    eval_loader = DataLoader(\n",
        "        eval_dataset,\n",
        "        batch_size=CFG.eval_batch_size,\n",
        "        num_workers=0,\n",
        "        pin_memory=True,\n",
        "        shuffle=False,\n",
        "    )\n",
        "\n",
        "    num_training_steps = CFG.epochs * len(combined_train_loader)\n",
        "    optimizer = optim.AdamW(model.parameters(), lr=CFG.lr)\n",
        "    scheduler = get_scheduler(name=CFG.scheduler, optimizer=optimizer, num_warmup_steps=0, num_training_steps=num_training_steps)\n",
        "\n",
        "    def train_one_epoch(model, scheduler, train_loader, optimizer):\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "        progress_bar = tqdm(train_loader, dynamic_ncols=True)\n",
        "\n",
        "        for step, batch in enumerate(progress_bar):\n",
        "            batch = to_device(batch, device)\n",
        "            outputs = model(\n",
        "                input_ids=batch[\"input_ids\"],\n",
        "                attention_mask=batch[\"attention_mask\"],\n",
        "                token_type_ids=batch[\"token_type_ids\"],\n",
        "                labels=batch[\"labels\"],\n",
        "            )\n",
        "            loss = outputs.loss\n",
        "            loss.backward()\n",
        "\n",
        "            text = f\"step {step}, loss: {loss:.5f}\"\n",
        "            progress_bar.set_description(text)\n",
        "            optimizer.step()\n",
        "            scheduler.step()\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "    def evaluate_model(model, test_loader):\n",
        "        model.eval()\n",
        "        predictions = []\n",
        "        labels = []\n",
        "        for batch in eval_loader:\n",
        "            batch = to_device(batch, device)\n",
        "            with torch.no_grad():\n",
        "                outputs = model(\n",
        "                    input_ids=batch[\"input_ids\"],\n",
        "                    attention_mask=batch[\"attention_mask\"],\n",
        "                    token_type_ids=batch[\"token_type_ids\"],\n",
        "                    labels=batch[\"labels\"],\n",
        "                )\n",
        "\n",
        "            logits = outputs.logits\n",
        "            prediction = torch.argmax(logits, dim=-1)\n",
        "            predictions.append(prediction.cpu().numpy())\n",
        "            labels.append(batch[\"labels\"].cpu().numpy())\n",
        "\n",
        "        predictions = np.concatenate(predictions)\n",
        "        labels = np.concatenate(labels)\n",
        "        return f1.compute(predictions=predictions, references=labels, average='macro')\n",
        "\n",
        "    model.to(device)\n",
        "    for i in range(CFG.classification_epochs):\n",
        "        train_one_epoch(model, scheduler, train_loader, optimizer)\n",
        "        accuracy = evaluate_model(model, eval_loader)\n",
        "        print(f'Epoch {i+1} {accuracy}')\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "jXAjWlZFmBlm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cHRDuKNBj5IW"
      },
      "outputs": [],
      "source": [
        "def main():\n",
        "\n",
        "    if CFG.if_wandb:\n",
        "        wandb.init(\n",
        "            name=CFG.name,\n",
        "            project=CFG.project,\n",
        "            config=cfg\n",
        "        )\n",
        "\n",
        "    raw_dataset = load_dataset('InternationalOlympiadAI/NLP_problem_raw', token=read_access_token)\n",
        "    classification_dataset = load_dataset('InternationalOlympiadAI/NLP_problem', token=read_access_token)\n",
        "\n",
        "    device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "\n",
        "    tokenizer = train_tokenizer(raw_dataset)\n",
        "\n",
        "    pseudo_base_model_name = pretrain(raw_dataset, tokenizer)\n",
        "    fine_tuned_model = fine_tine(pseudo_base_model_name, tokenizer, classification_dataset, device)\n",
        "\n",
        "    pseudo_model = AutoModelForSequenceClassification.from_pretrained(\n",
        "        pseudo_base_model_name, num_labels=CFG.num_classes\n",
        "    ).cuda()\n",
        "\n",
        "    model = AutoModelForSequenceClassification.from_pretrained(\n",
        "        fine_tuned_model, num_labels=CFG.num_classes\n",
        "    ).cuda()\n",
        "\n",
        "    pseudo_data, confidences = pseudo_get_data(raw_dataset, transform_raw, pseudo_model, device)\n",
        "    pseudo_labeled_tokens = pseudo_data.with_transform(transform)\n",
        "\n",
        "    tokenized_data = classification_dataset.with_transform(transform)\n",
        "\n",
        "    train_dataset = tokenized_data[\"train\"]\n",
        "    eval_dataset = tokenized_data[\"dev\"]\n",
        "\n",
        "    combined_train_dataset = concatenate_datasets([pseudo_labeled_tokens, train_dataset])\n",
        "\n",
        "    combined_train_loader = DataLoader(\n",
        "        combined_train_dataset,\n",
        "        batch_size=CFG.train_batch_size,\n",
        "        num_workers=0,\n",
        "        pin_memory=True,\n",
        "        shuffle=True,\n",
        "        drop_last=True,\n",
        "    )\n",
        "\n",
        "    eval_loader = DataLoader(\n",
        "        eval_dataset,\n",
        "        batch_size=CFG.eval_batch_size,\n",
        "        num_workers=0,\n",
        "        pin_memory=True,\n",
        "        shuffle=False,\n",
        "    )\n",
        "\n",
        "    num_training_steps = CFG.epochs * len(combined_train_loader)\n",
        "    optimizer = optim.AdamW(model.parameters(), lr=CFG.lr)\n",
        "    scheduler = get_scheduler(name=CFG.scheduler, optimizer=optimizer, num_warmup_steps=0, num_training_steps=num_training_steps)\n",
        "\n",
        "    def train_one_epoch(model, scheduler, train_loader, optimizer):\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "        progress_bar = tqdm(train_loader, dynamic_ncols=True)\n",
        "\n",
        "        for step, batch in enumerate(progress_bar):\n",
        "            batch = to_device(batch, device)\n",
        "            outputs = model(\n",
        "                input_ids=batch[\"input_ids\"],\n",
        "                attention_mask=batch[\"attention_mask\"],\n",
        "                token_type_ids=batch[\"token_type_ids\"],\n",
        "                labels=batch[\"labels\"],\n",
        "            )\n",
        "            loss = outputs.loss\n",
        "            loss.backward()\n",
        "\n",
        "            text = f\"step {step}, loss: {loss:.5f}\"\n",
        "            progress_bar.set_description(text)\n",
        "\n",
        "            if CFG.if_wandb:\n",
        "                wandb.log(\n",
        "                    {\n",
        "                        \"train_loss\": loss,\n",
        "                        \"lr\": optimizer.param_groups[0][\"lr\"],\n",
        "                        \"step\": step,\n",
        "                    }\n",
        "                )\n",
        "            optimizer.step()\n",
        "            scheduler.step()\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "    def evaluate_model(model, test_loader):\n",
        "        model.eval()\n",
        "        predictions = []\n",
        "        labels = []\n",
        "        for batch in eval_loader:\n",
        "            batch = to_device(batch, device)\n",
        "            with torch.no_grad():\n",
        "                outputs = model(\n",
        "                    input_ids=batch[\"input_ids\"],\n",
        "                    attention_mask=batch[\"attention_mask\"],\n",
        "                    token_type_ids=batch[\"token_type_ids\"],\n",
        "                    labels=batch[\"labels\"],\n",
        "                )\n",
        "\n",
        "            logits = outputs.logits\n",
        "            prediction = torch.argmax(logits, dim=-1)\n",
        "            predictions.append(prediction.cpu().numpy())\n",
        "            labels.append(batch[\"labels\"].cpu().numpy())\n",
        "\n",
        "        predictions = np.concatenate(predictions)\n",
        "        labels = np.concatenate(labels)\n",
        "        return f1.compute(predictions=predictions, references=labels, average='macro')\n",
        "\n",
        "    # Train and evaluate the model\n",
        "    model.to(device)\n",
        "    for i in range(CFG.epochs):\n",
        "        train_one_epoch(model, scheduler, combined_train_loader, optimizer)\n",
        "        accuracy = evaluate_model(model, eval_loader)\n",
        "        if CFG.if_wandb:\n",
        "            wandb.log(\n",
        "                {\n",
        "                    \"epoch\": i+1,\n",
        "                    \"accuracy\": accuracy\n",
        "                }\n",
        "            )\n",
        "        print(f'Epoch {i+1} {accuracy}')\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zs02eqO9jQBx"
      },
      "outputs": [],
      "source": [
        "model = main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lMNqHsF0olYA"
      },
      "outputs": [],
      "source": [
        "# model.push_to_hub(\n",
        "#     f\"ioai2024japan/{CFG.name}\",\n",
        "#     token=userdata.get('hf_write'), private=True\n",
        "# )"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "machine_shape": "hm",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}